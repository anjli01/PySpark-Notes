{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/14_Working_with_Dates_and_Timestamps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mastering date and timestamp manipulation in PySpark is fundamental for Data Engineers, especially when dealing with time-series data, ETL processes, and analytical reporting. PySpark offers a robust set of functions in `pyspark.sql.functions` to handle these operations efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Concepts for Dates and Timestamps\n",
        "\n",
        "*   **DateType vs. TimestampType**:\n",
        "    *   `DateType`: Represents a date (year, month, day) without time or time zone information.\n",
        "    *   `TimestampType`: Represents a point in time (year, month, day, hour, minute, second, microsecond) with time zone information (internally stored as UTC).\n",
        "*   **Time Zones**:\n",
        "    *   Spark stores `TimestampType` values internally as **UTC (Coordinated Universal Time)**.\n",
        "    *   When you read/write or display timestamps, Spark converts them to/from the session's configured time zone (`spark.sql.session.timeZone`).\n",
        "    *   Always be explicit with time zones during ingestion and presentation to avoid ambiguity.\n",
        "    *   **Best Practice**: Store all timestamp data in UTC in your data lake/warehouse. Convert to local time zones only at the presentation layer (e.g., during reporting).\n",
        "\n",
        "---\n",
        "\n",
        "### Essential PySpark Date & Timestamp Functions\n",
        "\n",
        "These functions are available in `pyspark.sql.functions`.\n",
        "\n",
        "| Function                      | Description                                                                                              | Notes for Beginners                                                                                                                                                                             |\n",
        "| :---------------------------- | :------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `to_date(col, format=None)`   | Converts a `StringType` column to `DateType`. An optional `format` string can be provided.               | Always specify a `format` string if your input date string isn't in a standard `yyyy-MM-dd` format. If `format` is omitted, Spark will try to infer, but explicit is better.                   |\n",
        "| `to_timestamp(col, format=None, timeZone=None)` | Converts a `StringType` to `TimestampType`.                                                        | Similar to `to_date`, specify `format`. Use `timeZone` to interpret the input string in a specific time zone before converting to internal UTC.                                            |\n",
        "| `cast(\"timestamp\")`           | Converts a `StringType` (e.g., `yyyy-MM-dd HH:mm:ss`) to `TimestampType`.                                | A simpler way for standard formats. If the string format doesn't match, `to_timestamp` with an explicit format is more reliable.                                                              |\n",
        "| `current_date()`              | Returns the current date as a `DateType` literal.                                                        | Useful for adding a \"snapshot date\" to your data.                                                                                                                                               |\n",
        "| `current_timestamp()`         | Returns the current timestamp as a `TimestampType` literal.                                              | Useful for auditing or \"last updated\" fields. Reflects the session's configured time zone when displayed, but internally is UTC.                                                               |\n",
        "| `datediff(end_date, start_date)` | Returns the number of days between two `DateType` columns.                                               | `end_date - start_date`. The result is an `IntegerType`.                                                                                                                                       |\n",
        "| `months_between(ts1, ts2, roundOff=True)` | Returns the number of months between two timestamps.                                                     | Can handle fractional months. `roundOff=True` rounds to 8 decimal places.                                                                                                                       |\n",
        "| `add_months(start_date, num_months)` | Returns the date that is `num_months` after `start_date`.                                                | `num_months` can be negative to subtract months.                                                                                                                                                |\n",
        "| `date_add(start_date, num_days)` | Adds `num_days` to `start_date`.                                                                         | `num_days` can be negative.                                                                                                                                                                     |\n",
        "| `date_sub(start_date, num_days)` | Subtracts `num_days` from `start_date`.                                                                  | Equivalent to `date_add(start_date, -num_days)`.                                                                                                                                                |\n",
        "| `date_format(date, format)`   | Formats a `DateType` or `TimestampType` column to a `StringType` according to the specified format.        | Essential for presenting dates/times in a user-friendly or specific reporting format.                                                                                                           |\n",
        "| `year(col)`, `month(col)`, `dayofmonth(col)`, `dayofweek(col)` | Extracts year, month, day of month, day of week (1=Sunday, 7=Saturday) from a date/timestamp. | Useful for time-based aggregations (e.g., sales by month, weekly trends). `dayofweek` is important for calendrical operations.                                                                   |\n",
        "| `hour(col)`, `minute(col)`, `second(col)` | Extracts hour, minute, second from a timestamp.                                                          | Useful for detailed time-series analysis or grouping by time of day.                                                                                                                            |\n",
        "| `from_unixtime(unixtime_col, format)` | Converts Unix timestamp (seconds since epoch) to a formatted string.                                     | Unix timestamps are common in system logs. Use this to make them human-readable.                                                                                                                |\n",
        "| `unix_timestamp(ts_col, format)` | Converts a timestamp string (with optional format) to a Unix timestamp (seconds since epoch).            | Useful when you need to store timestamps as numerical values or for compatibility with other systems that use Unix timestamps.                                                                  |\n",
        "\n",
        "---\n",
        "### PySpark Date & Timestamp Manipulation Examples\n"
      ],
      "metadata": {
        "id": "B6IFsSVHJk4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"DateTimestampFunctions\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (\"2023-01-01\", \"2023-01-31 10:30:00\"),\n",
        "    (\"2023-02-15\", \"2023-02-15 14:00:00\"),\n",
        "    (\"2022-12-25\", \"2022-12-25 23:59:59\")\n",
        "]\n",
        "columns = [\"event_date_str\", \"event_ts_str\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "\n",
        "# --- 1. Convert string to DateType and TimestampType ---\n",
        "print(\"\\n--- After converting to DateType and TimestampType ---\")\n",
        "df_converted = df.withColumn(\"event_date\", F.to_date(F.col(\"event_date_str\"))) \\\n",
        "                 .withColumn(\"event_ts\", F.col(\"event_ts_str\").cast(\"timestamp\"))\n",
        "\n",
        "df_converted.show(truncate=False)\n",
        "df_converted.printSchema()\n",
        "\n",
        "# --- 2. Current Date and Timestamp ---\n",
        "print(\"\\n--- Current Date and Timestamp ---\")\n",
        "spark.range(1).select(\n",
        "    F.current_date().alias(\"Today\"),\n",
        "    F.current_timestamp().alias(\"Now\")\n",
        ").show(truncate=False)\n",
        "\n",
        "# --- 3. Date Difference ---\n",
        "print(\"\\n--- Date difference (days between current date and event_date) ---\")\n",
        "df_converted.withColumn(\n",
        "    \"days_since_event\",\n",
        "    F.datediff(F.current_date(), F.col(\"event_date\"))\n",
        ").show()\n",
        "\n",
        "# --- 4. Add/Subtract Months and Days ---\n",
        "print(\"\\n--- Date after adding 3 months and 5 days ---\")\n",
        "df_converted.withColumn(\"date_plus_3_months\", F.add_months(F.col(\"event_date\"), 3)) \\\n",
        "            .withColumn(\"date_plus_5_days\", F.date_add(F.col(\"event_date\"), 5)) \\\n",
        "            .withColumn(\"date_minus_5_days\", F.date_sub(F.col(\"event_date\"), 5)) \\\n",
        "            .show()\n",
        "\n",
        "# --- 5. Date and Timestamp Formatting ---\n",
        "print(\"\\n--- Formatted Date and Timestamp ---\")\n",
        "df_converted.withColumn(\"formatted_date\", F.date_format(F.col(\"event_date\"), \"yyyy/MM/dd\")) \\\n",
        "            .withColumn(\"formatted_ts\", F.date_format(F.col(\"event_ts\"), \"MM-dd-yyyy HH:mm:ss\")) \\\n",
        "            .show(truncate=False)\n",
        "\n",
        "# --- 6. Extracting Components ---\n",
        "print(\"\\n--- Date/Time Components ---\")\n",
        "df_converted.select(\n",
        "    F.col(\"event_date\"),\n",
        "    F.year(F.col(\"event_date\")).alias(\"Year\"),\n",
        "    F.month(F.col(\"event_date\")).alias(\"Month\"),\n",
        "    F.dayofmonth(F.col(\"event_date\")).alias(\"DayOfMonth\"),\n",
        "    F.dayofweek(F.col(\"event_date\")).alias(\"DayOfWeek\"), # 1=Sunday, 7=Saturday\n",
        "    F.hour(F.col(\"event_ts\")).alias(\"Hour\"),\n",
        "    F.minute(F.col(\"event_ts\")).alias(\"Minute\"),\n",
        "    F.second(F.col(\"event_ts\")).alias(\"Second\")\n",
        ").show()\n",
        "\n",
        "# --- 7. Filtering by Date Range ---\n",
        "print(\"\\n--- Filtering for tasks completed in February 2023 ---\")\n",
        "tasks_data = [\n",
        "    (\"TaskA\", \"2023-01-10\"),\n",
        "    (\"TaskB\", \"2023-02-20\"),\n",
        "    (\"TaskC\", \"2023-01-05\"),\n",
        "    (\"TaskD\", \"2023-03-01\"),\n",
        "    (\"TaskE\", \"2023-02-28\")\n",
        "]\n",
        "tasks_cols = [\"Task\", \"CompletionDate\"]\n",
        "df_tasks = spark.createDataFrame(tasks_data, tasks_cols) \\\n",
        "                .withColumn(\"CompletionDate\", F.to_date(F.col(\"CompletionDate\")))\n",
        "\n",
        "df_tasks.show()\n",
        "df_tasks.printSchema()\n",
        "\n",
        "df_tasks.filter(\n",
        "    (F.col(\"CompletionDate\") >= \"2023-02-01\") & \\\n",
        "    (F.col(\"CompletionDate\") <= \"2023-02-28\")\n",
        ").show()\n",
        "\n",
        "# Filtering for tasks completed within 30 days of a specific date\n",
        "target_date = F.to_date(F.lit(\"2023-02-15\"))\n",
        "print(f\"\\n--- Filtering for tasks completed within 30 days of 2023-02-15 ---\")\n",
        "df_tasks.filter(\n",
        "    F.datediff(F.col(\"CompletionDate\"), target_date).between(-30, 30)\n",
        ").show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTn4nx2cniw4",
        "outputId": "89f440b3-70dc-4e47-f12b-c8a0cda4905b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+--------------+-------------------+\n",
            "|event_date_str|event_ts_str       |\n",
            "+--------------+-------------------+\n",
            "|2023-01-01    |2023-01-31 10:30:00|\n",
            "|2023-02-15    |2023-02-15 14:00:00|\n",
            "|2022-12-25    |2022-12-25 23:59:59|\n",
            "+--------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- event_date_str: string (nullable = true)\n",
            " |-- event_ts_str: string (nullable = true)\n",
            "\n",
            "\n",
            "--- After converting to DateType and TimestampType ---\n",
            "+--------------+-------------------+----------+-------------------+\n",
            "|event_date_str|event_ts_str       |event_date|event_ts           |\n",
            "+--------------+-------------------+----------+-------------------+\n",
            "|2023-01-01    |2023-01-31 10:30:00|2023-01-01|2023-01-31 10:30:00|\n",
            "|2023-02-15    |2023-02-15 14:00:00|2023-02-15|2023-02-15 14:00:00|\n",
            "|2022-12-25    |2022-12-25 23:59:59|2022-12-25|2022-12-25 23:59:59|\n",
            "+--------------+-------------------+----------+-------------------+\n",
            "\n",
            "root\n",
            " |-- event_date_str: string (nullable = true)\n",
            " |-- event_ts_str: string (nullable = true)\n",
            " |-- event_date: date (nullable = true)\n",
            " |-- event_ts: timestamp (nullable = true)\n",
            "\n",
            "\n",
            "--- Current Date and Timestamp ---\n",
            "+----------+--------------------------+\n",
            "|Today     |Now                       |\n",
            "+----------+--------------------------+\n",
            "|2025-10-02|2025-10-02 14:20:33.697055|\n",
            "+----------+--------------------------+\n",
            "\n",
            "\n",
            "--- Date difference (days between current date and event_date) ---\n",
            "+--------------+-------------------+----------+-------------------+----------------+\n",
            "|event_date_str|       event_ts_str|event_date|           event_ts|days_since_event|\n",
            "+--------------+-------------------+----------+-------------------+----------------+\n",
            "|    2023-01-01|2023-01-31 10:30:00|2023-01-01|2023-01-31 10:30:00|            1005|\n",
            "|    2023-02-15|2023-02-15 14:00:00|2023-02-15|2023-02-15 14:00:00|             960|\n",
            "|    2022-12-25|2022-12-25 23:59:59|2022-12-25|2022-12-25 23:59:59|            1012|\n",
            "+--------------+-------------------+----------+-------------------+----------------+\n",
            "\n",
            "\n",
            "--- Date after adding 3 months and 5 days ---\n",
            "+--------------+-------------------+----------+-------------------+------------------+----------------+-----------------+\n",
            "|event_date_str|       event_ts_str|event_date|           event_ts|date_plus_3_months|date_plus_5_days|date_minus_5_days|\n",
            "+--------------+-------------------+----------+-------------------+------------------+----------------+-----------------+\n",
            "|    2023-01-01|2023-01-31 10:30:00|2023-01-01|2023-01-31 10:30:00|        2023-04-01|      2023-01-06|       2022-12-27|\n",
            "|    2023-02-15|2023-02-15 14:00:00|2023-02-15|2023-02-15 14:00:00|        2023-05-15|      2023-02-20|       2023-02-10|\n",
            "|    2022-12-25|2022-12-25 23:59:59|2022-12-25|2022-12-25 23:59:59|        2023-03-25|      2022-12-30|       2022-12-20|\n",
            "+--------------+-------------------+----------+-------------------+------------------+----------------+-----------------+\n",
            "\n",
            "\n",
            "--- Formatted Date and Timestamp ---\n",
            "+--------------+-------------------+----------+-------------------+--------------+-------------------+\n",
            "|event_date_str|event_ts_str       |event_date|event_ts           |formatted_date|formatted_ts       |\n",
            "+--------------+-------------------+----------+-------------------+--------------+-------------------+\n",
            "|2023-01-01    |2023-01-31 10:30:00|2023-01-01|2023-01-31 10:30:00|2023/01/01    |01-31-2023 10:30:00|\n",
            "|2023-02-15    |2023-02-15 14:00:00|2023-02-15|2023-02-15 14:00:00|2023/02/15    |02-15-2023 14:00:00|\n",
            "|2022-12-25    |2022-12-25 23:59:59|2022-12-25|2022-12-25 23:59:59|2022/12/25    |12-25-2022 23:59:59|\n",
            "+--------------+-------------------+----------+-------------------+--------------+-------------------+\n",
            "\n",
            "\n",
            "--- Date/Time Components ---\n",
            "+----------+----+-----+----------+---------+----+------+------+\n",
            "|event_date|Year|Month|DayOfMonth|DayOfWeek|Hour|Minute|Second|\n",
            "+----------+----+-----+----------+---------+----+------+------+\n",
            "|2023-01-01|2023|    1|         1|        1|  10|    30|     0|\n",
            "|2023-02-15|2023|    2|        15|        4|  14|     0|     0|\n",
            "|2022-12-25|2022|   12|        25|        1|  23|    59|    59|\n",
            "+----------+----+-----+----------+---------+----+------+------+\n",
            "\n",
            "\n",
            "--- Filtering for tasks completed in February 2023 ---\n",
            "+-----+--------------+\n",
            "| Task|CompletionDate|\n",
            "+-----+--------------+\n",
            "|TaskA|    2023-01-10|\n",
            "|TaskB|    2023-02-20|\n",
            "|TaskC|    2023-01-05|\n",
            "|TaskD|    2023-03-01|\n",
            "|TaskE|    2023-02-28|\n",
            "+-----+--------------+\n",
            "\n",
            "root\n",
            " |-- Task: string (nullable = true)\n",
            " |-- CompletionDate: date (nullable = true)\n",
            "\n",
            "+-----+--------------+\n",
            "| Task|CompletionDate|\n",
            "+-----+--------------+\n",
            "|TaskB|    2023-02-20|\n",
            "|TaskE|    2023-02-28|\n",
            "+-----+--------------+\n",
            "\n",
            "\n",
            "--- Filtering for tasks completed within 30 days of 2023-02-15 ---\n",
            "+-----+--------------+\n",
            "| Task|CompletionDate|\n",
            "+-----+--------------+\n",
            "|TaskB|    2023-02-20|\n",
            "|TaskD|    2023-03-01|\n",
            "|TaskE|    2023-02-28|\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Handling Time Zones and Formats\n",
        "\n",
        "Time zone handling is critical for data consistency, especially with distributed data.\n",
        "\n",
        "*   **`spark.sql.session.timeZone`**: This configuration property sets the default time zone for the Spark session. All timestamp operations that don't explicitly specify a time zone will use this. The default is typically the JVM's local time zone.\n",
        "*   **Internal Storage is UTC**: Remember, Spark stores `TimestampType` values internally as UTC. Any display or conversion to a string format will apply the session's time zone or an explicitly provided one.\n",
        "\n",
        "#### Example (Python):"
      ],
      "metadata": {
        "id": "W2t2eXLlXdpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzpRD2n9D-G9",
        "outputId": "2c1c9c3f-7110-42bb-e757-be9e9a164645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session Time Zone: America/Los_Angeles\n",
            "\n",
            "Original string DataFrame:\n",
            "+-------------------+\n",
            "|timestamp_str      |\n",
            "+-------------------+\n",
            "|2023-07-19 10:00:00|\n",
            "|2023-07-19 15:00:00|\n",
            "+-------------------+\n",
            "\n",
            "\n",
            "Original timestamps (converted to session timezone display):\n",
            "+-------------------+-------------------+\n",
            "|timestamp_str      |timestamp          |\n",
            "+-------------------+-------------------+\n",
            "|2023-07-19 10:00:00|2023-07-19 10:00:00|\n",
            "|2023-07-19 15:00:00|2023-07-19 15:00:00|\n",
            "+-------------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- timestamp_str: string (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            "\n",
            "\n",
            "Timestamp interpreted as GMT, then displayed in session TZ (America/Los_Angeles):\n",
            "+-------------------+-------------------+\n",
            "|timestamp_str      |ts_gmt             |\n",
            "+-------------------+-------------------+\n",
            "|2023-07-19 10:00:00|2023-07-19 10:00:00|\n",
            "|2023-07-19 15:00:00|2023-07-19 15:00:00|\n",
            "+-------------------+-------------------+\n",
            "\n",
            "\n",
            "Session Time Zone changed to: UTC\n",
            "\n",
            "Original timestamps (displayed in new session TZ - UTC):\n",
            "+-------------------+-------------------+\n",
            "|timestamp_str      |timestamp          |\n",
            "+-------------------+-------------------+\n",
            "|2023-07-19 10:00:00|2023-07-19 17:00:00|\n",
            "|2023-07-19 15:00:00|2023-07-19 22:00:00|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TimezoneHandling\").getOrCreate()\n",
        "\n",
        "# --- Initial Setup ---\n",
        "# Set session time zone for demonstration\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
        "print(f\"Session Time Zone: {spark.conf.get('spark.sql.session.timeZone')}\")\n",
        "\n",
        "data = [(\"2023-07-19 10:00:00\",), (\"2023-07-19 15:00:00\",)]\n",
        "columns = [\"timestamp_str\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "print(\"\\nOriginal string DataFrame:\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "# --- 1. Convert string to TimestampType (assuming input is in session timezone or no TZ specified) ---\n",
        "# Spark will interpret 'timestamp_str' as being in 'America/Los_Angeles' (session TZ)\n",
        "# and convert it to UTC internally. When shown, it's converted back to 'America/Los_Angeles'.\n",
        "df_ts = df.withColumn(\"timestamp\", F.col(\"timestamp_str\").cast(\"timestamp\"))\n",
        "print(\"\\nOriginal timestamps (converted to session timezone display):\")\n",
        "df_ts.show(truncate=False)\n",
        "df_ts.printSchema()\n",
        "\n",
        "# --- 2. Demonstrate different time zones during conversion ---\n",
        "# Interpret the input string \"2023-07-19 10:00:00\" as belonging to GMT\n",
        "# and convert to a timestamp. This internal UTC value will then be displayed in the session TZ.\n",
        "df_tz_gmt = df.withColumn(\n",
        "    \"ts_gmt\",\n",
        "    F.to_timestamp(F.col(\"timestamp_str\"), \"yyyy-MM-dd HH:mm:ss\") # Removed the extra \"GMT\" argument\n",
        ")\n",
        "print(\"\\nTimestamp interpreted as GMT, then displayed in session TZ (America/Los_Angeles):\")\n",
        "df_tz_gmt.show(truncate=False)\n",
        "\n",
        "# --- 3. Revert session time zone to UTC for comparison ---\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
        "print(f\"\\nSession Time Zone changed to: {spark.conf.get('spark.sql.session.timeZone')}\")\n",
        "\n",
        "# Now observe how the same internal timestamp value is displayed differently\n",
        "print(\"\\nOriginal timestamps (displayed in new session TZ - UTC):\")\n",
        "df_ts.show(truncate=False) # The internal value of 'timestamp' column hasn't changed, only its display\n",
        "\n",
        "# --- 4. Format timestamp to specific time zone strings ---\n",
        "# You can explicitly specify a timezone for formatting\n",
        "# print(\"\\nFormatting timestamp to specific time zone strings:\")\n",
        "# df_ts.withColumn(\"formatted_utc\", F.date_format(F.col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss z\")) \\\n",
        "#      .withColumn(\"formatted_la\", F.date_format(F.col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss z\", \"America/Los_Angeles\")) \\\n",
        "#      .withColumn(\"formatted_london\", F.date_format(F.col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss z\", \"Europe/London\")) \\\n",
        "#      .show(truncate=False)\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Principles for Beginner Data Engineers\n",
        "\n",
        "*   **Be Explicit**: Always specify date/timestamp formats when converting strings. If omitted, Spark might guess incorrectly, leading to `null` values or wrong conversions.\n",
        "*   **UTC for Storage**: Store all `TimestampType` data in your underlying data storage (Parquet, Delta, etc.) in UTC. This standardizes your data and avoids ambiguities.\n",
        "*   **Local Time Zones at Edges**: Only convert to a specific local time zone when ingesting data from a source that provides local timestamps, or when presenting data to users for reporting.\n",
        "*   **Leverage Built-in Functions**: PySpark's `pyspark.sql.functions` module is optimized for performance and is the preferred way to handle date and time operations, rather than UDFs (User Defined Functions) which can be slower."
      ],
      "metadata": {
        "id": "sr7kywTHXeBP"
      }
    }
  ]
}