{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/17_UDFs_and_Pandas_UDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UDFs (User-Defined Functions) Overview\n",
        "\n",
        "UDFs allow you to extend Spark's functionality by writing custom logic. There are primarily three styles for Python UDFs: Regular UDFs, Pandas UDFs (Vectorized UDFs), and the new Spark 3.5+ Python UDFs.\n",
        "\n",
        "### 1. Regular Python UDFs\n",
        "\n",
        "Regular UDFs operate on a **row-by-row basis**, similar to a standard Python function applied iteratively to each element of a column.\n",
        "\n",
        "*   **Definition:**\n",
        "    *   Define a Python function.\n",
        "    *   Wrap it with `pyspark.sql.functions.udf`.\n",
        "    *   **Crucially, specify the `returnType`** (e.g., `StringType()`, `IntegerType()`) for Spark's optimizer.\n",
        "\n",
        "*   **Example (Python):**"
      ],
      "metadata": {
        "id": "8x-ndqvvJrgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RegularUDFs\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Define a Python function\n",
        "def age_category(age):\n",
        "    if age < 25:\n",
        "        return \"Young\"\n",
        "    elif age >= 25 and age < 35:\n",
        "        return \"Mid\"\n",
        "    else:\n",
        "        return \"Senior\"\n",
        "\n",
        "# Register the Python function as a UDF\n",
        "# Specify the returnType: StringType() in this case\n",
        "age_category_udf = udf(age_category, StringType())\n",
        "\n",
        "# Apply the UDF to the DataFrame\n",
        "print(\"\\nDataFrame with Age Category (using Regular UDF):\")\n",
        "df.withColumn(\"AgeCategory\", age_category_udf(col(\"Age\"))).show()\n",
        "\n",
        "# Another UDF example: simple addition\n",
        "def add_one(value):\n",
        "    return value + 1\n",
        "\n",
        "add_one_udf = udf(add_one, IntegerType())\n",
        "\n",
        "print(\"\\nDataFrame with Age + 1 (using Regular UDF):\")\n",
        "df.withColumn(\"AgePlusOne\", add_one_udf(col(\"Age\"))).show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFGWW1x9p57E",
        "outputId": "a5e3b1a8-3bd0-4bc5-858e-6971acf51d70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "DataFrame with Age Category (using Regular UDF):\n",
            "+-------+---+-----------+\n",
            "|   Name|Age|AgeCategory|\n",
            "+-------+---+-----------+\n",
            "|  Alice| 30|        Mid|\n",
            "|    Bob| 25|        Mid|\n",
            "|Charlie| 35|     Senior|\n",
            "+-------+---+-----------+\n",
            "\n",
            "\n",
            "DataFrame with Age + 1 (using Regular UDF):\n",
            "+-------+---+----------+\n",
            "|   Name|Age|AgePlusOne|\n",
            "+-------+---+----------+\n",
            "|  Alice| 30|        31|\n",
            "|    Bob| 25|        26|\n",
            "|Charlie| 35|        36|\n",
            "+-------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Performance Drawbacks (Why they are generally discouraged):**\n",
        "    1.  **Serialization/Deserialization Overhead:** Spark's optimized internal format (Tungsten) must convert data to Python objects for the UDF, then convert results back. This constant conversion is slow.\n",
        "    2.  **Optimization Barrier (Black Box):** Spark's Catalyst Optimizer cannot \"see inside\" a regular UDF. It treats it as a black box, preventing powerful optimizations like predicate pushdown, column pruning, and code generation.\n",
        "    3.  **Python Process Overhead:** Each Spark executor launches separate Python processes to run UDFs, incurring overhead for launching, managing, and communicating between the JVM (Spark core) and Python.\n",
        "    4.  **No Vectorization:** Regular UDFs process data one row at a time, which is inefficient compared to vectorized operations that process data in batches.\n",
        "\n",
        "*   **When to Use:** Sparingly, and only when a built-in Spark SQL function cannot achieve the desired logic, or for simple, non-performance-critical transformations on small datasets.\n",
        "\n",
        "### 2. Pandas UDFs (Vectorized UDFs)\n",
        "\n",
        "Pandas UDFs significantly improve performance by leveraging **Apache Arrow** for efficient data transfer and processing data in **batches** using Pandas Series/DataFrames.\n",
        "\n",
        "*   **Definition:**\n",
        "    *   Define a Python function that takes one or more Pandas Series as input.\n",
        "    *   It **must return a Pandas Series of the same length** as the input.\n",
        "    *   Decorate the function with `@pandas_udf(returnType)`, specifying a Spark SQL data type.\n",
        "    *   Operates on `pandas.Series` objects, enabling vectorized operations.\n",
        "\n",
        "*   **Example (Python):**\n"
      ],
      "metadata": {
        "id": "8NI0sLjCXpMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpO_YSbFD62B",
        "outputId": "81400207-dd7e-4933-f248-ee6b1df3aef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "DataFrame with Age * 10 (using Pandas UDF):\n",
            "+-------+---+-----------+\n",
            "|   Name|Age|AgeTimesTen|\n",
            "+-------+---+-----------+\n",
            "|  Alice| 30|        300|\n",
            "|    Bob| 25|        250|\n",
            "|Charlie| 35|        350|\n",
            "+-------+---+-----------+\n",
            "\n",
            "\n",
            "DataFrame with Age Category (using Pandas UDF):\n",
            "+-------+---+-----------------+\n",
            "|   Name|Age|AgeCategoryPandas|\n",
            "+-------+---+-----------------+\n",
            "|  Alice| 30|              Mid|\n",
            "|    Bob| 25|              Mid|\n",
            "|Charlie| 35|           Senior|\n",
            "+-------+---+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, pandas_udf\n",
        "from pyspark.sql.types import LongType, StringType\n",
        "import pandas as pd\n",
        "import numpy as np # numpy is typically used with pandas for such operations\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PandasUDFs\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Define a Pandas UDF (Series to Series)\n",
        "@pandas_udf(LongType()) # Return type is LongType\n",
        "def multiply_by_ten(series: pd.Series) -> pd.Series:\n",
        "    return series * 10\n",
        "\n",
        "print(\"\\nDataFrame with Age * 10 (using Pandas UDF):\")\n",
        "df.withColumn(\"AgeTimesTen\", multiply_by_ten(col(\"Age\"))).show()\n",
        "\n",
        "# Another Pandas UDF: apply a more complex string transformation\n",
        "@pandas_udf(StringType())\n",
        "def categorize_age_pandas(ages: pd.Series) -> pd.Series:\n",
        "    conditions = [\n",
        "        ages < 25,\n",
        "        (ages >= 25) & (ages < 35),\n",
        "        ages >= 35\n",
        "    ]\n",
        "    choices = [\"Young\", \"Mid\", \"Senior\"]\n",
        "    return pd.Series(np.select(conditions, choices, default=\"Unknown\"))\n",
        "\n",
        "print(\"\\nDataFrame with Age Category (using Pandas UDF):\")\n",
        "df.withColumn(\"AgeCategoryPandas\", categorize_age_pandas(col(\"Age\"))).show()\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Performance Advantages (Vectorized, Arrow-based):**\n",
        "    1.  **Vectorized Execution:** Processes batches of rows (as Pandas Series) instead of one row at a time. This allows efficient use of Pandas' optimized operations (often implemented in C).\n",
        "    2.  **Apache Arrow Optimization:** Spark uses Apache Arrow, an in-memory columnar data format, for efficient data transfer between the JVM and Python processes. This minimizes serialization/deserialization overhead.\n",
        "    3.  **Catalyst Integration (Improved):** While still somewhat of an optimization barrier, the batch processing nature and Arrow integration allow Spark to manage data transfer more efficiently, leading to better performance than regular UDFs.\n",
        "\n",
        "*   **When to Use:**\n",
        "    *   When existing Python libraries (like NumPy, Pandas, Scikit-learn) have functions well-suited for vectorized operations and are not available as Spark SQL functions.\n",
        "    *   For complex custom logic that benefits significantly from batch processing.\n",
        "\n",
        "### 3. New Python UDFs (Spark 3.5+ Style)\n",
        "\n",
        "With Spark 3.5+, a more streamlined way to define Python UDFs was introduced, leveraging **Python type annotations**. Spark automatically infers the return type and often provides an optimized execution path similar to Pandas UDFs internally.\n",
        "\n",
        "*   **Key Features:**\n",
        "    1.  **Type Annotation Driven:** Spark infers the return type from the Python type hints in the function signature.\n",
        "    2.  **Optimized Execution:** Spark can use a more optimized, vectorized execution path (similar to Pandas UDFs) if the types and operations allow, potentially using Apache Arrow.\n",
        "    3.  **Simpler Syntax:** No need for explicit `udf()` wrapper or `@pandas_udf` decorator for basic UDFs.\n",
        "\n",
        "*   **Example (Python - Spark 3.5+ style):**\n"
      ],
      "metadata": {
        "id": "9_JWMkK3XpmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "# No need to import udf or pandas_udf explicitly for this style if using Spark 3.5+\n",
        "from pyspark.sql.types import IntegerType, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"NewPythonUDFs\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Define a Python function with type annotations\n",
        "# Spark 3.5+ will automatically infer this as a UDF\n",
        "def increment_age(age: int) -> int:\n",
        "    return age + 1\n",
        "\n",
        "# Apply the function directly to the DataFrame.\n",
        "# Spark's internal mechanisms will convert this to a UDF.\n",
        "print(\"\\nDataFrame with Age + 1 (using new Python UDF style):\")\n",
        "df.withColumn(\"AgePlusOne\", increment_age(col(\"Age\"))).show()\n",
        "\n",
        "# Another example with conditional logic using Spark's when/otherwise\n",
        "def get_age_status(age_col): # Pass the column object\n",
        "    return when(age_col < 25, \"Young\") \\\n",
        "           .when((age_col >= 25) & (age_col < 35), \"Adult\") \\\n",
        "           .otherwise(\"Senior\")\n",
        "\n",
        "print(\"\\nDataFrame with Age Status (using new Python UDF style):\")\n",
        "# Apply the function to the column\n",
        "df.withColumn(\"AgeStatus\", get_age_status(col(\"Age\"))).show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFTACXoAXp5H",
        "outputId": "5cddb524-07ca-457e-ad0d-26ff14092a76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "DataFrame with Age + 1 (using new Python UDF style):\n",
            "+-------+---+----------+\n",
            "|   Name|Age|AgePlusOne|\n",
            "+-------+---+----------+\n",
            "|  Alice| 30|        31|\n",
            "|    Bob| 25|        26|\n",
            "|Charlie| 35|        36|\n",
            "+-------+---+----------+\n",
            "\n",
            "\n",
            "DataFrame with Age Status (using new Python UDF style):\n",
            "+-------+---+---------+\n",
            "|   Name|Age|AgeStatus|\n",
            "+-------+---+---------+\n",
            "|  Alice| 30|    Adult|\n",
            "|    Bob| 25|    Adult|\n",
            "|Charlie| 35|   Senior|\n",
            "+-------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Important Note:** While this style simplifies syntax and often improves performance (relative to regular UDFs), for the **absolute best performance**, especially with heavy numerical computation, **explicitly using `@pandas_udf` for vectorized operations is still a strong choice**. Always profile your UDFs to determine the most efficient approach for your specific workload.\n",
        "\n",
        "---\n",
        "\n",
        "## UDF Comparison Table for Beginner Data Engineers\n",
        "\n",
        "| Feature / UDF Type       | Regular UDF (Legacy)    | Pandas UDF (Vectorized)      | Spark 3.5+ Python UDF (Type Annotated) |\n",
        "| :----------------------- | :---------------------- | :--------------------------- | :--------------------------------------- |\n",
        "| **Processing Mode**      | Row-by-row              | Batch (Pandas Series)        | Batch (often internally vectorized)      |\n",
        "| **Syntax**               | `udf(func, returnType)` | `@pandas_udf(returnType)`    | Type annotations `(arg: type) -> type`   |\n",
        "| **Data Transfer**        | Python object conversion | Apache Arrow                 | Apache Arrow (when vectorized)           |\n",
        "| **Serialization Overhead**| High                    | Low                          | Low (when vectorized)                    |\n",
        "| **Catalyst Optimization**| No (Black Box)          | Improved (still limited)     | Often Optimized (Spark can \"see\" types)  |\n",
        "| **Performance**          | Low                     | High                         | Medium to High                           |\n",
        "| **Key Use Case**         | Avoid if possible       | Complex vectorized logic, NumPy/Pandas/Scikit-learn integration | Simple custom logic with good performance, general use in modern Spark |\n",
        "| **Recommendation**       | Avoid for large scale    | Preferred for high-perf and vectorized tasks | Good default for new UDFs, but profile!   |\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Mastering UDFs is part of becoming a proficient Spark Data Engineer. Always prioritize built-in Spark SQL functions when available, as they are typically the most optimized. When custom logic is needed, favor Pandas UDFs or the new Spark 3.5+ style for better performance. Remember to always profile your Spark jobs to ensure your chosen UDF approach is performing optimally for your specific workload."
      ],
      "metadata": {
        "id": "oXBLMA5FXqJQ"
      }
    }
  ]
}