{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/12_Sorting%2C_Filtering%2C_Distinct%2C_DropDuplicates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are concise notes for Beginner Data Engineers, based on the provided content and enhanced with practical considerations:\n",
        "\n",
        "---\n",
        "\n",
        "# Spark Transformations for Data Engineers: Clean, Sort, Deduplicate\n",
        "\n",
        "As a Data Engineer, mastering Spark transformations is fundamental for efficient data cleaning, organization, and preparation. Pay close attention to the performance implications, especially the difference between wide and narrow transformations.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Sorting Data (`sort`, `orderBy`)\n",
        "\n",
        "Sorting orders DataFrame rows by one or more columns.\n",
        "\n",
        "*   **Aliases**: `sort(*cols, asc=True)` and `orderBy(*cols, asc=True)` are interchangeable.\n",
        "*   **Direction**:\n",
        "    *   **Ascending (default)**: `df.sort(\"Age\")`\n",
        "    *   **Descending**: Use `col(\"column_name\").desc()`, e.g., `df.sort(col(\"Age\").desc())`\n",
        "    *   **Multiple Columns**: Specify order for each, e.g., `df.orderBy(\"Age\", col(\"Name\").desc())`\n",
        "*   **Transformation Type**: **Wide Transformation** (requires data shuffling).\n",
        "\n",
        "**Example (Python):**"
      ],
      "metadata": {
        "id": "OYUQCSt1JgZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"SortFilterDistinct\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [(\"Alice\", 30, \"NY\"),\n",
        "        (\"Bob\", 25, \"LD\"),\n",
        "        (\"Charlie\", 35, \"NY\"),\n",
        "        (\"Alice\", 30, \"NY\"), # Duplicate row\n",
        "        (\"David\", 22, \"SF\"),\n",
        "        (\"Charlie\", 35, \"LA\")] # Charlie is duplicated on Name/Age, but not Name/Age/City\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "# +-------+---+----+\n",
        "# |   Name|Age|City|\n",
        "# +-------+---+----+\n",
        "# |  Alice| 30|  NY|\n",
        "# |    Bob| 25|  LD|\n",
        "# |Charlie| 35|  NY|\n",
        "# |  Alice| 30|  NY|\n",
        "# |  David| 22|  SF|\n",
        "# |Charlie| 35|  LA|\n",
        "# +-------+---+----+\n",
        "\n",
        "print(\"\\nSorted by Age (ascending):\")\n",
        "df.sort(\"Age\").show()\n",
        "\n",
        "print(\"\\nSorted by Age (asc), then Name (desc):\")\n",
        "df.orderBy(\"Age\", col(\"Name\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY75n-OYlJU-",
        "outputId": "4d1cd655-3dc1-4773-92a9-07855b8c3400"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|  Alice| 30|  NY|\n",
            "|    Bob| 25|  LD|\n",
            "|Charlie| 35|  NY|\n",
            "|  Alice| 30|  NY|\n",
            "|  David| 22|  SF|\n",
            "|Charlie| 35|  LA|\n",
            "+-------+---+----+\n",
            "\n",
            "\n",
            "Sorted by Age (ascending):\n",
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|  David| 22|  SF|\n",
            "|    Bob| 25|  LD|\n",
            "|  Alice| 30|  NY|\n",
            "|  Alice| 30|  NY|\n",
            "|Charlie| 35|  LA|\n",
            "|Charlie| 35|  NY|\n",
            "+-------+---+----+\n",
            "\n",
            "\n",
            "Sorted by Age (asc), then Name (desc):\n",
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|  David| 22|  SF|\n",
            "|    Bob| 25|  LD|\n",
            "|  Alice| 30|  NY|\n",
            "|  Alice| 30|  NY|\n",
            "|Charlie| 35|  LA|\n",
            "|Charlie| 35|  NY|\n",
            "+-------+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Filtering Data (`filter` / `where`)\n",
        "\n",
        "Filtering selects rows based on a specified condition.\n",
        "\n",
        "*   **Aliases**: `filter(condition)` and `where(condition)` are interchangeable.\n",
        "*   **Transformation Type**: **Narrow Transformation** (no shuffling required).\n",
        "\n",
        "**Example (Python):**"
      ],
      "metadata": {
        "id": "cYn5EybvXVKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dPhZkCSD_54",
        "outputId": "ab9ec537-f15e-4b16-c7f5-5e5f17ddfc6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtered for Age > 28:\n",
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|  Alice| 30|  NY|\n",
            "|Charlie| 35|  NY|\n",
            "|  Alice| 30|  NY|\n",
            "|Charlie| 35|  LA|\n",
            "+-------+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFiltered for Age > 28:\")\n",
        "df.filter(col(\"Age\") > 28).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Handling Duplicates (`distinct`, `dropDuplicates`)\n",
        "\n",
        "Removing duplicate rows is a critical data cleaning step.\n",
        "\n",
        "### `distinct()`\n",
        "\n",
        "*   Returns a new DataFrame with only unique rows, considering **all columns**.\n",
        "*   **Transformation Type**: **Wide Transformation** (requires global comparison and shuffling).\n",
        "\n",
        "### `dropDuplicates(subset=None)`\n",
        "\n",
        "*   Removes duplicate rows.\n",
        "*   **`subset=None`**: Considers **all columns** for duplication (similar to `distinct()`).\n",
        "*   **`subset=[column_names]`**: Considers *only* the specified columns for duplication, keeping the first occurrence.\n",
        "*   **Transformation Type**: **Wide Transformation** (requires shuffling).\n",
        "\n",
        "### ðŸš€ **Performance Preference for Data Engineers**\n",
        "\n",
        "`dropDuplicates(subset=...)` is generally **preferred over `distinct()`** for targeted de-duplication:\n",
        "\n",
        "*   **Efficient Shuffle**: When `subset` is specified, Spark only needs to shuffle data based on those columns, significantly reducing network I/O and data movement.\n",
        "*   **Reduced Overhead**: `distinct()` always processes all columns, even if not needed for uniqueness, incurring higher overhead.\n",
        "*   **Targeted Control**: Provides fine-grained control over which columns define uniqueness.\n",
        "\n",
        "**Example (Python):**"
      ],
      "metadata": {
        "id": "l84AQ5cYXVjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDistinct rows (considering all columns):\")\n",
        "df.distinct().show()\n",
        "\n",
        "print(\"\\nDrop duplicates (on all columns):\")\n",
        "df.dropDuplicates().show()\n",
        "\n",
        "print(\"\\nDrop duplicates on 'Name' and 'Age' (keep first occurrence):\")\n",
        "df.dropDuplicates(subset=[\"Name\", \"Age\"]).show()\n",
        "# +-------+---+----+\n",
        "# |   Name|Age|City|\n",
        "# +-------+---+----+\n",
        "# |  David| 22|  SF|\n",
        "# |    Bob| 25|  LD|\n",
        "# |  Alice| 30|  NY| # Keeps the first Alice, 30 occurrence\n",
        "# |Charlie| 35|  NY| # Keeps the first Charlie, 35 occurrence\n",
        "# +-------+---+----+\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6tbDRUCXVzc",
        "outputId": "51ad7a26-2eb0-42e5-c67d-642d94f2371d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distinct rows (considering all columns):\n",
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|    Bob| 25|  LD|\n",
            "|Charlie| 35|  NY|\n",
            "|  Alice| 30|  NY|\n",
            "|  David| 22|  SF|\n",
            "|Charlie| 35|  LA|\n",
            "+-------+---+----+\n",
            "\n",
            "\n",
            "Drop duplicates (on all columns):\n",
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|    Bob| 25|  LD|\n",
            "|Charlie| 35|  NY|\n",
            "|  Alice| 30|  NY|\n",
            "|  David| 22|  SF|\n",
            "|Charlie| 35|  LA|\n",
            "+-------+---+----+\n",
            "\n",
            "\n",
            "Drop duplicates on 'Name' and 'Age' (keep first occurrence):\n",
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|  Alice| 30|  NY|\n",
            "|    Bob| 25|  LD|\n",
            "|Charlie| 35|  NY|\n",
            "|  David| 22|  SF|\n",
            "+-------+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Understanding Wide vs. Narrow Transformations (KEY CONCEPT for DEs)\n",
        "\n",
        "Understanding these types is vital for writing performant Spark applications.\n",
        "\n",
        "| Feature             | Narrow Transformations                                      | Wide Transformations (Shuffles)                                      |\n",
        "| :------------------ | :---------------------------------------------------------- | :------------------------------------------------------------------- |\n",
        "| **Input to Output** | Each input partition contributes to **at most one** output partition. | Each input partition can contribute to **multiple** output partitions.   |\n",
        "| **Data Shuffling**  | **NO data shuffling** across the network.                   | **REQUIRES data shuffling** across the network (expensive).           |\n",
        "| **Performance**     | Generally **faster** (no network I/O).                    | Can be significantly **slower** (network I/O, serialization/deserialization, disk I/O). |\n",
        "| **Examples**        | `filter()`, `map()`, `withColumn()`, `select()`, `unionAll()` | `groupBy()`, `orderBy()`, `sort()`, `distinct()`, `dropDuplicates()`, `repartition()`, `join()` (unless broadcast). |\n",
        "| **DE Imperative**   | **Prefer these** when possible.                           | **Optimize them** when unavoidable.                                      |\n",
        "\n",
        "**Why it matters for Data Engineers:**\n",
        "\n",
        "*   **Cost of Shuffles**: Network I/O, disk I/O, and CPU overhead make shuffles the most expensive operations in Spark.\n",
        "*   **Data Skew**: Shuffles can lead to imbalanced partitions (data skew), causing some tasks to run significantly longer and creating bottlenecks.\n",
        "*   **Optimization Strategies**:\n",
        "    *   **Minimize Shuffles**: Restructure logic to perform narrow ops first.\n",
        "    *   **Proper Partitioning**: Ensure data is optimally partitioned before wide operations.\n",
        "    *   **`spark.sql.shuffle.partitions`**: Tune this configuration (default 200) to balance partition size and task overhead.\n",
        "    *   **Broadcast Joins**: Use for small lookup tables to avoid a shuffle during `join` operations.\n"
      ],
      "metadata": {
        "id": "F7dvMjQvXWDl"
      }
    }
  ]
}