{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/06_Data_Types_%26_Schema.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Types & Schema Fundamentals\n",
        "\n",
        "**Why it's important:** Defining a correct schema ensures data integrity, optimizes storage and computation, and helps Spark perform better by knowing exactly how to handle data.\n",
        "\n",
        "Spark's `pyspark.sql.types` module provides classes for programmatically defining schemas.\n",
        "\n",
        "*   **`StructType`**: Represents the schema of a DataFrame, which is essentially a list of `StructField` objects. It defines the structure of a row.\n",
        "*   **`StructField`**: Represents a column within a `StructType`. It specifies the column's name, data type, and nullability.\n",
        "    *   **Arguments:**\n",
        "\n",
        "| Argument       | Type      | Description                                                    |\n",
        "| :------------- | :-------- | :------------------------------------------------------------- |\n",
        "| `name`         | `string`  | The name of the column.                                        |\n",
        "| `dataType`     | `DataType`| The data type of the column (e.g., `StringType()`, `IntegerType()`, `BooleanType()`). |\n",
        "| `nullable`     | `boolean` | Whether the column can contain null values (`True` for nullable, `False` otherwise). **Best practice: Be explicit with `False` for critical ID columns.** |\n",
        "\n",
        "*   **`ArrayType(elementType, containsNull)`**: Represents an array (list) of elements of a specific type.\n",
        "    *   `elementType`: The data type of the elements in the array.\n",
        "    *   `containsNull`: Whether the array can contain null elements.\n",
        "*   **`MapType(keyType, valueType, valueContainsNull)`**: Represents a map (dictionary) with key-value pairs.\n",
        "    *   `keyType`: The data type of the keys.\n",
        "    *   `valueType`: The data type of the values.\n",
        "    *   `valueContainsNull`: Whether the map values can be null.\n",
        "\n",
        "#### Example: Defining a Complex Schema (Python)"
      ],
      "metadata": {
        "id": "zj5AVRC-JSv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataTypesSchema\").getOrCreate()\n",
        "\n",
        "# Define a complex schema for employee data\n",
        "schema = StructType([\n",
        "    StructField(\"employee_id\", IntegerType(), False), # Not nullable, typically an ID\n",
        "    StructField(\"name\", StructType([                       # Nested StructType for name\n",
        "        StructField(\"first\", StringType(), True),\n",
        "        StructField(\"last\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"skills\", ArrayType(StringType(), True), True), # Array of strings for skills\n",
        "    StructField(\"contact_info\", MapType(StringType(), StringType(), True), True) # Map with string keys and values\n",
        "])\n",
        "\n",
        "# Create sample data that conforms to the schema\n",
        "data = [(1, (\"Alice\", \"Smith\"), [\"Python\", \"Spark\"], {\"email\": \"alice@example.com\", \"phone\": \"123-456-7890\"}),\n",
        "        (2, (\"Bob\", \"Johnson\"), [\"Java\", \"SQL\"], {\"email\": \"bob@example.com\"}),\n",
        "        (3, (\"Charlie\", None), [], {}) # Example with null in nested struct, empty array, empty map\n",
        "       ]\n",
        "\n",
        "# Create a DataFrame with the defined schema\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "print(\"DataFrame with complex schema:\")\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbRQ7JGGfrPB",
        "outputId": "0882db11-7db1-4e86-8abd-84c1cddac0e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with complex schema:\n",
            "+-----------+---------------+---------------+---------------------------------------------------+\n",
            "|employee_id|name           |skills         |contact_info                                       |\n",
            "+-----------+---------------+---------------+---------------------------------------------------+\n",
            "|1          |{Alice, Smith} |[Python, Spark]|{phone -> 123-456-7890, email -> alice@example.com}|\n",
            "|2          |{Bob, Johnson} |[Java, SQL]    |{email -> bob@example.com}                         |\n",
            "|3          |{Charlie, NULL}|[]             |{}                                                 |\n",
            "+-----------+---------------+---------------+---------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- employee_id: integer (nullable = false)\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- first: string (nullable = true)\n",
            " |    |-- last: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- contact_info: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### `inferSchema` vs. Manual Schema Definition\n",
        "\n",
        "When reading data from external files (like CSV, JSON), Spark can either try to guess the schema (`inferSchema=True`) or you can provide it explicitly.\n",
        "\n",
        "| Feature         | `inferSchema=True` (Automatic)                                    | Manual Schema Definition                                       |\n",
        "| :-------------- | :---------------------------------------------------------------- | :------------------------------------------------------------- |\n",
        "| **Description** | Spark samples data to guess column names and data types.          | You explicitly define `StructType` and `StructField` for each column. |\n",
        "| **Pros**        | Convenient, less manual work, quick for exploration/ad-hoc analysis. | **Fast** (no extra pass over data), **Robust** (prevents unexpected type changes), enables early detection of mismatches. |\n",
        "| **Cons**        | **Slow** (requires an extra pass), **May infer incorrect types** (e.g., all strings if data is dirty, or `StringType` for IDs with some `NULL`s). Not suitable for production. | More verbose, requires knowing the schema beforehand.           |\n",
        "| **Usage**       | `spark.read.csv(\"path.csv\", header=True, inferSchema=True)`       | `spark.read.csv(\"path.csv\", header=True, schema=my_schema)`    |\n",
        "| **Best for**    | Quick prototyping, initial data exploration.                      | **Production ETL pipelines**, ensuring data quality and pipeline stability. |\n",
        "\n",
        "#### Example: `inferSchema` vs. Manual Schema (Python)"
      ],
      "metadata": {
        "id": "ayqcrr1oW6ki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGcirnk7EDjc",
        "outputId": "d078958a-c142-44d6-b59c-cf6f2ce20e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Infer Schema ---\n",
            "DataFrame with Inferred Schema:\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+---+-------+----+--------+\n",
            "| id|   name| age|    city|\n",
            "+---+-------+----+--------+\n",
            "|  1|  Alice|  30|New York|\n",
            "|  2|    Bob|  25|  London|\n",
            "|  3|Charlie|  35|   Paris|\n",
            "|  4|  David|NULL|  Berlin|\n",
            "|  5|    Eve|NULL|  London|\n",
            "+---+-------+----+--------+\n",
            "\n",
            "\n",
            "--- 2. Define Schema Manually ---\n",
            "\n",
            "DataFrame with Manual Schema:\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+---+-------+----+--------+\n",
            "| id|   name| age|    city|\n",
            "+---+-------+----+--------+\n",
            "|  1|  Alice|  30|New York|\n",
            "|  2|    Bob|  25|  London|\n",
            "|  3|Charlie|  35|   Paris|\n",
            "|  4|  David|NULL|  Berlin|\n",
            "|  5|    Eve|NULL|  London|\n",
            "+---+-------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"InferVsManualSchema\").getOrCreate()\n",
        "\n",
        "# Create a dummy CSV file for demonstration\n",
        "csv_content = \"\"\"id,name,age,city\n",
        "1,Alice,30,New York\n",
        "2,Bob,25,London\n",
        "3,Charlie,35,Paris\n",
        "4,David,NULL,Berlin\n",
        "5,Eve,,London\"\"\" # Example with a truly empty age\n",
        "\n",
        "with open(\"people_data.csv\", \"w\") as f:\n",
        "    f.write(csv_content)\n",
        "\n",
        "print(\"--- 1. Infer Schema ---\")\n",
        "df_inferred = spark.read.csv(\"people_data.csv\", header=True, inferSchema=True)\n",
        "print(\"DataFrame with Inferred Schema:\")\n",
        "df_inferred.printSchema()\n",
        "df_inferred.show()\n",
        "# Notice: 'age' might be inferred as IntegerType or StringType/DoubleType depending on Spark version\n",
        "# and the presence of 'NULL'/'empty' values. 'NULL' (as a string) often leads to StringType.\n",
        "# An empty string \"\" might be inferred as null for IntegerType if inferSchema handles it gracefully,\n",
        "# but can sometimes lead to StringType if not. Here, Spark correctly infers 'age' as IntegerType\n",
        "# even with 'NULL' string or empty string, converting them to actual nulls.\n",
        "\n",
        "print(\"\\n--- 2. Define Schema Manually ---\")\n",
        "manual_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),            # id should be Integer, not nullable\n",
        "    StructField(\"name\", StringType(), True),            # name can be string\n",
        "    StructField(\"age\", IntegerType(), True),            # age should be Integer, expecting nulls\n",
        "    StructField(\"city\", StringType(), True)             # city can be string\n",
        "])\n",
        "\n",
        "df_manual = spark.read.csv(\"people_data.csv\", header=True, schema=manual_schema)\n",
        "print(\"\\nDataFrame with Manual Schema:\")\n",
        "df_manual.printSchema()\n",
        "df_manual.show()\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### `cast()` for Type Conversions\n",
        "\n",
        "The `cast()` function, typically used with `col()` or `df.withColumn()`, is essential for converting a column from one data type to another. This is crucial for data cleaning, transformation, and ensuring data integrity.\n",
        "\n",
        "*   **Syntax**: `col(\"column_name\").cast(TargetDataType())`\n",
        "*   **Behavior with invalid casts**: If a value cannot be cast to the target type (e.g., \"abc\" to `IntegerType()`), Spark will convert it to `null` without raising an error.\n",
        "\n",
        "#### Example: `cast()` for Type Conversions (Python)"
      ],
      "metadata": {
        "id": "UK88vaA-W7C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType, StringType, DoubleType, DateType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TypeConversions\").getOrCreate()\n",
        "\n",
        "data = [(\"1\", \"10.5\", \"2023-01-15\"),\n",
        "        (\"2\", \"20.7\", \"2023-02-20\"),\n",
        "        (\"3\", \"30.9\", \"2023-03-25\")]\n",
        "columns = [\"id_str\", \"price_str\", \"date_str\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame Schema:\")\n",
        "df.printSchema()\n",
        "df.show()\n",
        "\n",
        "# Cast 'id_str' to IntegerType\n",
        "df_casted = df.withColumn(\"id_int\", col(\"id_str\").cast(IntegerType()))\n",
        "\n",
        "# Cast 'price_str' to DoubleType\n",
        "df_casted = df_casted.withColumn(\"price_double\", col(\"price_str\").cast(DoubleType()))\n",
        "\n",
        "# Cast 'date_str' to DateType\n",
        "df_casted = df_casted.withColumn(\"date_date\", col(\"date_str\").cast(DateType()))\n",
        "\n",
        "print(\"\\nDataFrame After Casts:\")\n",
        "df_casted.printSchema()\n",
        "df_casted.show()\n",
        "\n",
        "# Handling invalid casts:\n",
        "# If a value cannot be cast, it will result in a null.\n",
        "data_invalid = [(\"1\", \"abc\"), (\"2\", \"123\")]\n",
        "df_invalid = spark.createDataFrame(data_invalid, [\"id\", \"num_str\"])\n",
        "\n",
        "print(\"\\nDataFrame with potential invalid casts:\")\n",
        "df_invalid.show()\n",
        "\n",
        "# Attempt to cast a non-numeric string to IntegerType\n",
        "df_invalid.withColumn(\"num_int\", col(\"num_str\").cast(IntegerType())).show() # \"abc\" will become null\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Zvjn8DW7Ve",
        "outputId": "a1247933-d4e6-4615-d7fe-1dea8d6cc384"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame Schema:\n",
            "root\n",
            " |-- id_str: string (nullable = true)\n",
            " |-- price_str: string (nullable = true)\n",
            " |-- date_str: string (nullable = true)\n",
            "\n",
            "+------+---------+----------+\n",
            "|id_str|price_str|  date_str|\n",
            "+------+---------+----------+\n",
            "|     1|     10.5|2023-01-15|\n",
            "|     2|     20.7|2023-02-20|\n",
            "|     3|     30.9|2023-03-25|\n",
            "+------+---------+----------+\n",
            "\n",
            "\n",
            "DataFrame After Casts:\n",
            "root\n",
            " |-- id_str: string (nullable = true)\n",
            " |-- price_str: string (nullable = true)\n",
            " |-- date_str: string (nullable = true)\n",
            " |-- id_int: integer (nullable = true)\n",
            " |-- price_double: double (nullable = true)\n",
            " |-- date_date: date (nullable = true)\n",
            "\n",
            "+------+---------+----------+------+------------+----------+\n",
            "|id_str|price_str|  date_str|id_int|price_double| date_date|\n",
            "+------+---------+----------+------+------------+----------+\n",
            "|     1|     10.5|2023-01-15|     1|        10.5|2023-01-15|\n",
            "|     2|     20.7|2023-02-20|     2|        20.7|2023-02-20|\n",
            "|     3|     30.9|2023-03-25|     3|        30.9|2023-03-25|\n",
            "+------+---------+----------+------+------------+----------+\n",
            "\n",
            "\n",
            "DataFrame with potential invalid casts:\n",
            "+---+-------+\n",
            "| id|num_str|\n",
            "+---+-------+\n",
            "|  1|    abc|\n",
            "|  2|    123|\n",
            "+---+-------+\n",
            "\n",
            "+---+-------+-------+\n",
            "| id|num_str|num_int|\n",
            "+---+-------+-------+\n",
            "|  1|    abc|   NULL|\n",
            "|  2|    123|    123|\n",
            "+---+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Schema Evolution Awareness\n",
        "\n",
        "Schema evolution is the ability to handle changes in the schema of data over time. This is a common challenge in big data systems, especially with semi-structured data or streaming data.\n",
        "\n",
        "Spark, particularly with formats like Parquet, is generally good at handling schema evolution.\n",
        "\n",
        "#### Common Scenarios and How Spark Handles Them:\n",
        "\n",
        "| Scenario            | Spark's Handling (General)                                                                             | Data Engineer's Responsibility                                     |\n",
        "| :------------------ | :----------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------- |\n",
        "| **Adding New Columns** | New columns are included in the DataFrame's schema. Older records (without new columns) will have `null` values for those columns. | Ensure downstream systems can handle `null`s for new columns.      |\n",
        "| **Dropping Columns** | Spark will ignore removed columns if reading with an existing schema. If `inferSchema` is used, the schema will update. | Update your reading schema if you want to explicitly exclude them. |\n",
        "| **Changing Data Types** | Compatible changes (e.g., `Integer` to `Long`) are often handled. Incompatible changes (e.g., `String` to `Integer` if strings aren't valid numbers) will likely lead to `null` values or errors. | **Critical:** Anticipate and manage type changes with `cast()` or explicit schema definitions. Test thoroughly! |\n",
        "| **Reordering Columns** | Spark (especially Parquet) generally handles column reordering gracefully as it identifies columns by name, not by position. | No major issues typically, but consistency is still good practice. |\n",
        "\n",
        "#### Best Practices for Schema Evolution:\n",
        "\n",
        "*   **Use Schema-Aware Formats**: Prefer formats like **Parquet** or **Avro** that are designed with schema evolution in mind. They store schema information with the data and handle additive changes efficiently.\n",
        "*   **Define Schema Manually (Always for Production)**: While `inferSchema` is convenient for exploration, explicitly defining your schema gives you control, prevents unexpected type changes, and catches issues early.\n",
        "*   **Handle Nulls Gracefully**: Be prepared for `null` values when new columns are added or when data quality issues lead to invalid casts.\n",
        "*   **Version Control Schemas**: In complex data pipelines, store and version your schemas alongside your code to track changes and facilitate rollbacks.\n",
        "*   **Monitor Data Quality**: Implement data quality checks (e.g., column data type checks, null checks) to detect unexpected schema changes or data type issues before they propagate.\n",
        "\n",
        "#### Example (Conceptual):\n",
        "\n",
        "Imagine you have a Parquet file with `(id INT, name STRING)`.\n",
        "Later, new data arrives with `(id INT, name STRING, age INT)`.\n",
        "\n",
        "When you read the combined data with Spark, the resulting DataFrame will have `(id INT, name STRING, age INT)`. The older records (which didn't have `age`) will have `null` for the `age` column.\n",
        "\n",
        "This \"awareness\" means that as a Data Engineer, you should anticipate these changes and design your pipeline to be resilient to them, preventing unexpected errors or data quality issues.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uvMVIFRBW7mr"
      }
    }
  ]
}