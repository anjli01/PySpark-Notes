{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/08_DataFrame_Joins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. DataFrame Joins\n",
        "\n",
        "Joining DataFrames is a fundamental operation for combining data from different sources based on common columns.\n",
        "\n",
        "### Types of Joins\n",
        "\n",
        "Spark supports various SQL-like join types:\n",
        "\n",
        "| Join Type     | Alias (if any)      | Description                                                                                                                                                                                                                                                                      | Common Use Case                                                                                                    |\n",
        "| :------------ | :------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------- |\n",
        "| **inner**     | (default)           | Returns rows when there is a match in *both* DataFrames.                                                                                                                                                                                                                         | Finding common records, combining related information where both sides must exist.                                 |\n",
        "| **left**      | `left_outer`        | Returns *all* rows from the left DataFrame, and the matched rows from the right DataFrame. If no match, `nulls` are introduced for columns from the right.                                                                                                                         | Preserving all records from the \"main\" (left) table and adding supplemental info from the right.                   |\n",
        "| **right**     | `right_outer`       | Returns *all* rows from the right DataFrame, and the matched rows from the left DataFrame. If no match, `nulls` are introduced for columns from the left.                                                                                                                          | Preserving all records from a secondary (right) table and adding supplemental info from the left.                  |\n",
        "| **outer**     | `full_outer`        | Returns *all* rows when there is a match in *either* DataFrame. If no match, `nulls` are introduced for columns from the non-matching DataFrame.                                                                                                                                | Combining two datasets where you want to keep all records from both, regardless of a match.                        |\n",
        "| **left_semi** |                     | Returns rows from the left DataFrame *where there is a match in the right DataFrame*. **Only returns columns from the left DataFrame.** It's like an INNER JOIN but only selecting columns from the left, effectively filtering the left DataFrame.                                 | Filtering a main table based on the existence of records in another table without bringing in the other table's data. |\n",
        "| **left_anti** |                     | Returns rows from the left DataFrame *where there is NO match in the right DataFrame*. Useful for finding missing records or non-existent relationships. **Only returns columns from the left DataFrame.**                                                                           | Identifying records in one table that do *not* have a corresponding entry in another (e.g., missing configurations). |\n",
        "\n",
        "### Code Examples (Python)"
      ],
      "metadata": {
        "id": "Ji713t_JJXV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameJoins\").getOrCreate()\n",
        "\n",
        "# --- Create two DataFrames ---\n",
        "employees_data = [\n",
        "    (\"Alice\", 1, \"HR\"),\n",
        "    (\"Bob\", 2, \"Sales\"),\n",
        "    (\"Charlie\", 3, \"IT\"),\n",
        "    (\"David\", 4, \"Marketing\")\n",
        "]\n",
        "employees_columns = [\"Name\", \"EmpID\", \"Department\"]\n",
        "employees_df = spark.createDataFrame(employees_data, employees_columns)\n",
        "print(\"Employees DataFrame:\")\n",
        "employees_df.show()\n",
        "\n",
        "departments_data = [\n",
        "    (1, \"HR\", \"New York\"),\n",
        "    (2, \"Sales\", \"London\"),\n",
        "    (5, \"Finance\", \"Paris\")  # EmpID 5 has no matching employee\n",
        "]\n",
        "departments_columns = [\"DeptID\", \"DeptName\", \"Location\"]\n",
        "departments_df = spark.createDataFrame(departments_data, departments_columns)\n",
        "print(\"Departments DataFrame:\")\n",
        "departments_df.show()\n",
        "\n",
        "# --- Inner Join ---\n",
        "print(\"\\nInner Join:\")\n",
        "employees_df.join(\n",
        "    departments_df,\n",
        "    employees_df.Department == departments_df.DeptName,\n",
        "    \"inner\"\n",
        ").show()\n",
        "\n",
        "# --- Left Join ---\n",
        "print(\"\\nLeft Join:\")\n",
        "employees_df.join(\n",
        "    departments_df,\n",
        "    employees_df.Department == departments_df.DeptName,\n",
        "    \"left\"\n",
        ").show()\n",
        "\n",
        "# --- Right Join ---\n",
        "print(\"\\nRight Join:\")\n",
        "employees_df.join(\n",
        "    departments_df,\n",
        "    employees_df.Department == departments_df.DeptName,\n",
        "    \"right\"\n",
        ").show()\n",
        "\n",
        "# --- Full Outer Join ---\n",
        "print(\"\\nFull Outer Join:\")\n",
        "employees_df.join(\n",
        "    departments_df,\n",
        "    employees_df.Department == departments_df.DeptName,\n",
        "    \"full_outer\"\n",
        ").show()\n",
        "\n",
        "# --- Left Semi Join ---\n",
        "# Only columns from left (employees_df), where a match exists in departments_df\n",
        "print(\"\\nLeft Semi Join (only columns from left, where match exists):\")\n",
        "employees_df.join(\n",
        "    departments_df,\n",
        "    employees_df.Department == departments_df.DeptName,\n",
        "    \"left_semi\"\n",
        ").show()\n",
        "\n",
        "# --- Left Anti Join ---\n",
        "# Rows in left (employees_df) that do NOT have a match in right (departments_df)\n",
        "print(\"\\nLeft Anti Join (rows in left NOT in right):\")\n",
        "employees_df.join(\n",
        "    departments_df,\n",
        "    employees_df.Department == departments_df.DeptName,\n",
        "    \"left_anti\"\n",
        ").show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK8tfPukiaJW",
        "outputId": "67204704-cb9f-45ef-9134-6b2578131dd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees DataFrame:\n",
            "+-------+-----+----------+\n",
            "|   Name|EmpID|Department|\n",
            "+-------+-----+----------+\n",
            "|  Alice|    1|        HR|\n",
            "|    Bob|    2|     Sales|\n",
            "|Charlie|    3|        IT|\n",
            "|  David|    4| Marketing|\n",
            "+-------+-----+----------+\n",
            "\n",
            "Departments DataFrame:\n",
            "+------+--------+--------+\n",
            "|DeptID|DeptName|Location|\n",
            "+------+--------+--------+\n",
            "|     1|      HR|New York|\n",
            "|     2|   Sales|  London|\n",
            "|     5| Finance|   Paris|\n",
            "+------+--------+--------+\n",
            "\n",
            "\n",
            "Inner Join:\n",
            "+-----+-----+----------+------+--------+--------+\n",
            "| Name|EmpID|Department|DeptID|DeptName|Location|\n",
            "+-----+-----+----------+------+--------+--------+\n",
            "|Alice|    1|        HR|     1|      HR|New York|\n",
            "|  Bob|    2|     Sales|     2|   Sales|  London|\n",
            "+-----+-----+----------+------+--------+--------+\n",
            "\n",
            "\n",
            "Left Join:\n",
            "+-------+-----+----------+------+--------+--------+\n",
            "|   Name|EmpID|Department|DeptID|DeptName|Location|\n",
            "+-------+-----+----------+------+--------+--------+\n",
            "|    Bob|    2|     Sales|     2|   Sales|  London|\n",
            "|  Alice|    1|        HR|     1|      HR|New York|\n",
            "|  David|    4| Marketing|  NULL|    NULL|    NULL|\n",
            "|Charlie|    3|        IT|  NULL|    NULL|    NULL|\n",
            "+-------+-----+----------+------+--------+--------+\n",
            "\n",
            "\n",
            "Right Join:\n",
            "+-----+-----+----------+------+--------+--------+\n",
            "| Name|EmpID|Department|DeptID|DeptName|Location|\n",
            "+-----+-----+----------+------+--------+--------+\n",
            "|Alice|    1|        HR|     1|      HR|New York|\n",
            "|  Bob|    2|     Sales|     2|   Sales|  London|\n",
            "| NULL| NULL|      NULL|     5| Finance|   Paris|\n",
            "+-----+-----+----------+------+--------+--------+\n",
            "\n",
            "\n",
            "Full Outer Join:\n",
            "+-------+-----+----------+------+--------+--------+\n",
            "|   Name|EmpID|Department|DeptID|DeptName|Location|\n",
            "+-------+-----+----------+------+--------+--------+\n",
            "|   NULL| NULL|      NULL|     5| Finance|   Paris|\n",
            "|  Alice|    1|        HR|     1|      HR|New York|\n",
            "|Charlie|    3|        IT|  NULL|    NULL|    NULL|\n",
            "|  David|    4| Marketing|  NULL|    NULL|    NULL|\n",
            "|    Bob|    2|     Sales|     2|   Sales|  London|\n",
            "+-------+-----+----------+------+--------+--------+\n",
            "\n",
            "\n",
            "Left Semi Join (only columns from left, where match exists):\n",
            "+-----+-----+----------+\n",
            "| Name|EmpID|Department|\n",
            "+-----+-----+----------+\n",
            "|Alice|    1|        HR|\n",
            "|  Bob|    2|     Sales|\n",
            "+-----+-----+----------+\n",
            "\n",
            "\n",
            "Left Anti Join (rows in left NOT in right):\n",
            "+-------+-----+----------+\n",
            "|   Name|EmpID|Department|\n",
            "+-------+-----+----------+\n",
            "|  David|    4| Marketing|\n",
            "|Charlie|    3|        IT|\n",
            "+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## II. Handling Duplicate Columns in Joins\n",
        "\n",
        "When joining DataFrames with common column names, Spark can produce duplicate columns in the result, which needs careful handling.\n",
        "\n",
        "### Problem\n",
        "\n",
        "If you join on columns with the same name without explicitly managing them, you might end up with ambiguous column names like `ID` and `ID`.\n",
        "\n",
        "### Solutions\n",
        "\n",
        "1.  **Specify join condition explicitly (`df1.col == df2.col`)**:\n",
        "    *   This keeps both columns (e.g., `df1.ID`, `df2.ID`).\n",
        "    *   You can then explicitly `drop()` one if it's no longer needed, or select/alias them.\n",
        "2.  **Use `on` parameter with a string or list of strings**:\n",
        "    *   If you join on columns with the *same name* in both DataFrames using `df.join(other_df, \"common_col_name\")` or `df.join(other_df, [\"col1\", \"col2\"])`, Spark will automatically combine them into a single column in the result.\n",
        "3.  **Alias columns before joining (`withColumnRenamed`)**:\n",
        "    *   Rename one of the conflicting columns in one DataFrame *before* the join operation to avoid name collisions.\n",
        "\n",
        "### Code Examples (Python)"
      ],
      "metadata": {
        "id": "udPKQ737XEF8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "559nxFkrECT5",
        "outputId": "de082940-2834-431f-9379-798824eb54d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df1 DataFrame:\n",
            "+---+------+\n",
            "| ID|Value1|\n",
            "+---+------+\n",
            "|  A|    10|\n",
            "|  B|    20|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Value1: long (nullable = true)\n",
            "\n",
            "df2 DataFrame:\n",
            "+---+------+\n",
            "| ID|Value2|\n",
            "+---+------+\n",
            "|  A|   100|\n",
            "|  B|   200|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Value2: long (nullable = true)\n",
            "\n",
            "\n",
            "Join on 'ID' string (Spark handles duplicate 'ID'):\n",
            "+---+------+------+\n",
            "| ID|Value1|Value2|\n",
            "+---+------+------+\n",
            "|  A|    10|   100|\n",
            "|  B|    20|   200|\n",
            "+---+------+------+\n",
            "\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Value1: long (nullable = true)\n",
            " |-- Value2: long (nullable = true)\n",
            "\n",
            "\n",
            "Join on 'ID' using boolean expression (duplicates 'ID'):\n",
            "+---+------+---+------+\n",
            "| ID|Value1| ID|Value2|\n",
            "+---+------+---+------+\n",
            "|  A|    10|  A|   100|\n",
            "|  B|    20|  B|   200|\n",
            "+---+------+---+------+\n",
            "\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Value1: long (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Value2: long (nullable = true)\n",
            "\n",
            "\n",
            "After dropping one of the duplicate 'ID' columns:\n",
            "+---+------+------+\n",
            "| ID|Value1|Value2|\n",
            "+---+------+------+\n",
            "|  A|    10|   100|\n",
            "|  B|    20|   200|\n",
            "+---+------+------+\n",
            "\n",
            "\n",
            "Selecting and aliasing to resolve duplicates:\n",
            "+----------+------+------+\n",
            "|CustomerID|Value1|Value2|\n",
            "+----------+------+------+\n",
            "|         A|    10|   100|\n",
            "|         B|    20|   200|\n",
            "+----------+------+------+\n",
            "\n",
            "root\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- Value1: long (nullable = true)\n",
            " |-- Value2: long (nullable = true)\n",
            "\n",
            "\n",
            "Alias 'ID' in df2 before joining:\n",
            "+---+------+------+------+\n",
            "| ID|Value1|ID_df2|Value2|\n",
            "+---+------+------+------+\n",
            "|  A|    10|     A|   100|\n",
            "|  B|    20|     B|   200|\n",
            "+---+------+------+------+\n",
            "\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Value1: long (nullable = true)\n",
            " |-- ID_df2: string (nullable = true)\n",
            " |-- Value2: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"HandlingDuplicateColumns\").getOrCreate()\n",
        "\n",
        "# --- Create two DataFrames with common 'ID' column ---\n",
        "df1_data = [(\"A\", 10), (\"B\", 20)]\n",
        "df1_cols = [\"ID\", \"Value1\"]\n",
        "df1 = spark.createDataFrame(df1_data, df1_cols)\n",
        "print(\"df1 DataFrame:\")\n",
        "df1.show()\n",
        "df1.printSchema()\n",
        "\n",
        "df2_data = [(\"A\", 100), (\"B\", 200)]\n",
        "df2_cols = [\"ID\", \"Value2\"]\n",
        "df2 = spark.createDataFrame(df2_data, df2_cols)\n",
        "print(\"df2 DataFrame:\")\n",
        "df2.show()\n",
        "df2.printSchema()\n",
        "\n",
        "# --- Case 1: Join on common column name using a string (Spark handles it) ---\n",
        "print(\"\\nJoin on 'ID' string (Spark handles duplicate 'ID'):\")\n",
        "df_joined_str = df1.join(df2, \"ID\") # or on=[\"ID\"]\n",
        "df_joined_str.show()\n",
        "df_joined_str.printSchema() # Notice 'ID' appears only once\n",
        "\n",
        "# --- Case 2: Join on common column name using a boolean expression ('ID' is duplicated) ---\n",
        "print(\"\\nJoin on 'ID' using boolean expression (duplicates 'ID'):\")\n",
        "df_joined_bool = df1.join(df2, df1.ID == df2.ID)\n",
        "df_joined_bool.show()\n",
        "df_joined_bool.printSchema() # Notice 'ID' appears twice (df1.ID, df2.ID)\n",
        "\n",
        "# --- To resolve duplicate columns after boolean join: ---\n",
        "\n",
        "# Option A: Drop one of the duplicate columns\n",
        "print(\"\\nAfter dropping one of the duplicate 'ID' columns:\")\n",
        "df_joined_bool.drop(df2.ID).show()\n",
        "\n",
        "# Option B: Select specific columns and alias\n",
        "print(\"\\nSelecting and aliasing to resolve duplicates:\")\n",
        "df_resolved = df1.join(df2, df1.ID == df2.ID) \\\n",
        "    .select(df1.ID.alias(\"CustomerID\"), df1.Value1, df2.Value2) # Alias one ID\n",
        "df_resolved.show()\n",
        "df_resolved.printSchema()\n",
        "\n",
        "# --- Case 3: Alias column before joining ---\n",
        "print(\"\\nAlias 'ID' in df2 before joining:\")\n",
        "df2_renamed = df2.withColumnRenamed(\"ID\", \"ID_df2\")\n",
        "df_aliased_join = df1.join(df2_renamed, df1.ID == df2_renamed.ID_df2)\n",
        "df_aliased_join.show()\n",
        "df_aliased_join.printSchema() # Now ID and ID_df2 are distinct\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## III. Broadcast Joins\n",
        "\n",
        "A **broadcast join** (also known as a map-side join) is a crucial optimization technique in Spark for joins involving a *small DataFrame* and a *large DataFrame*.\n",
        "\n",
        "### How it Works\n",
        "\n",
        "1.  Spark \"broadcasts\" (sends) the *smaller* DataFrame to all executor nodes.\n",
        "2.  Each executor then holds a copy of the smaller DataFrame in its memory.\n",
        "3.  When the join operation occurs, each partition of the *larger* DataFrame can directly join with the broadcasted smaller DataFrame without requiring a costly shuffle of the large DataFrame.\n",
        "\n",
        "### Benefits\n",
        "\n",
        "*   **Significant performance improvement**: Eliminates the expensive shuffle phase for the larger DataFrame, which is typically the bottleneck in joins.\n",
        "*   **Reduced network I/O**: Less data needs to be transferred across the network, as the large DataFrame doesn't move.\n",
        "\n",
        "### When it Triggers\n",
        "\n",
        "Spark's Catalyst Optimizer automatically decides whether to perform a broadcast join based on the size of the DataFrames.\n",
        "\n",
        "1.  **Automatic Broadcasting**:\n",
        "    *   **`spark.sql.autoBroadcastJoinThreshold`**: This configuration property determines the maximum size (in bytes) of a DataFrame that will be broadcast.\n",
        "    *   The default value is typically `10MB` (10 * 1024 * 1024 bytes).\n",
        "    *   If a DataFrame's size is **less than or equal to** this threshold, Spark *might* broadcast it.\n",
        "2.  **Manually Forcing Broadcast**:\n",
        "    *   You can explicitly tell Spark to broadcast a DataFrame using `pyspark.sql.functions.broadcast()`.\n",
        "    *   This is useful if Spark's automatic threshold isn't ideal for your specific use case, or if you know a DataFrame is small but Spark's statistics haven't caught up. Use with caution for very large DataFrames, as it can lead to out-of-memory errors on executors.\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "Always aim for broadcast joins when one side of the join is significantly smaller than the other. Monitor the Spark UI to confirm if broadcast joins are happening as expected (`BroadcastHashJoin` or `BroadcastNestedLoopJoin` in the plan).\n",
        "\n",
        "### Code Examples (Python)"
      ],
      "metadata": {
        "id": "c4YwQXKgXEf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast, col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"BroadcastJoin\").getOrCreate()\n",
        "\n",
        "# --- Configure the broadcast join threshold (e.g., to 1MB for demonstration) ---\n",
        "# Reset to default 10MB later, or stop SparkSession\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 1 * 1024 * 1024) # 1MB\n",
        "\n",
        "# --- Create a small DataFrame (will be broadcasted) ---\n",
        "small_df_data = [(1, \"Apple\"), (2, \"Banana\"), (3, \"Orange\")]\n",
        "small_df_cols = [\"ID\", \"Fruit\"]\n",
        "small_df = spark.createDataFrame(small_df_data, small_df_cols)\n",
        "print(\"Small DataFrame:\")\n",
        "small_df.show()\n",
        "\n",
        "# --- Create a large DataFrame (simulated by generating more rows) ---\n",
        "large_df_data = [(i % 3 + 1, f\"User_{i}\") for i in range(100000)] # 100,000 rows\n",
        "large_df_cols = [\"FruitID\", \"UserName\"]\n",
        "large_df = spark.createDataFrame(large_df_data, large_df_cols)\n",
        "print(\"Large DataFrame (first 5 rows):\")\n",
        "large_df.show(5)\n",
        "\n",
        "# --- Perform the join - Spark will likely broadcast small_df automatically ---\n",
        "print(\"\\nAutomatic Broadcast Join (check Spark UI for details):\")\n",
        "joined_df_auto = large_df.join(small_df, large_df.FruitID == small_df.ID, \"inner\")\n",
        "joined_df_auto.show(5)\n",
        "joined_df_auto.explain() # Look for \"BroadcastHashJoin\" or \"BroadcastNestedLoopJoin\" in the plan\n",
        "\n",
        "# --- Manually force broadcast (even if it's larger than threshold, use with caution) ---\n",
        "print(\"\\nManual Broadcast Join (forced):\")\n",
        "joined_df_forced = large_df.join(broadcast(small_df), large_df.FruitID == small_df.ID, \"inner\")\n",
        "joined_df_forced.show(5)\n",
        "joined_df_forced.explain() # Should explicitly show BroadcastHashJoin\n",
        "\n",
        "# --- Reset the threshold to default or stop SparkSession ---\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024) # Reset to default 10MB\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejFk5c0tXEwR",
        "outputId": "9bb07478-f6a8-4e26-b86a-229f9d5dc5ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small DataFrame:\n",
            "+---+------+\n",
            "| ID| Fruit|\n",
            "+---+------+\n",
            "|  1| Apple|\n",
            "|  2|Banana|\n",
            "|  3|Orange|\n",
            "+---+------+\n",
            "\n",
            "Large DataFrame (first 5 rows):\n",
            "+-------+--------+\n",
            "|FruitID|UserName|\n",
            "+-------+--------+\n",
            "|      1|  User_0|\n",
            "|      2|  User_1|\n",
            "|      3|  User_2|\n",
            "|      1|  User_3|\n",
            "|      2|  User_4|\n",
            "+-------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Automatic Broadcast Join (check Spark UI for details):\n",
            "+-------+--------+---+-----+\n",
            "|FruitID|UserName| ID|Fruit|\n",
            "+-------+--------+---+-----+\n",
            "|      1|  User_0|  1|Apple|\n",
            "|      1|  User_3|  1|Apple|\n",
            "|      1|  User_6|  1|Apple|\n",
            "|      1|  User_9|  1|Apple|\n",
            "|      1| User_12|  1|Apple|\n",
            "+-------+--------+---+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- SortMergeJoin [FruitID#373L], [ID#360L], Inner\n",
            "   :- Sort [FruitID#373L ASC NULLS FIRST], false, 0\n",
            "   :  +- Exchange hashpartitioning(FruitID#373L, 200), ENSURE_REQUIREMENTS, [plan_id=1436]\n",
            "   :     +- Filter isnotnull(FruitID#373L)\n",
            "   :        +- Scan ExistingRDD[FruitID#373L,UserName#374]\n",
            "   +- Sort [ID#360L ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(ID#360L, 200), ENSURE_REQUIREMENTS, [plan_id=1437]\n",
            "         +- Filter isnotnull(ID#360L)\n",
            "            +- Scan ExistingRDD[ID#360L,Fruit#361]\n",
            "\n",
            "\n",
            "\n",
            "Manual Broadcast Join (forced):\n",
            "+-------+--------+---+------+\n",
            "|FruitID|UserName| ID| Fruit|\n",
            "+-------+--------+---+------+\n",
            "|      1|  User_0|  1| Apple|\n",
            "|      2|  User_1|  2|Banana|\n",
            "|      3|  User_2|  3|Orange|\n",
            "|      1|  User_3|  1| Apple|\n",
            "|      2|  User_4|  2|Banana|\n",
            "+-------+--------+---+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [FruitID#373L], [ID#360L], Inner, BuildRight, false\n",
            "   :- Filter isnotnull(FruitID#373L)\n",
            "   :  +- Scan ExistingRDD[FruitID#373L,UserName#374]\n",
            "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=1524]\n",
            "      +- Filter isnotnull(ID#360L)\n",
            "         +- Scan ExistingRDD[ID#360L,Fruit#361]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}