{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/03_DataFrame_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame Basics\n",
        "\n",
        "`DataFrames` are the primary abstraction for working with structured data in modern Spark applications. They represent a distributed collection of data organized into named columns, conceptually similar to a table in a relational database.\n",
        "\n",
        "### 1. Creating DataFrames\n",
        "\n",
        "DataFrames can be created from various sources.\n",
        "\n",
        "#### A. From Lists (Python)\n",
        "\n",
        "You can create a DataFrame from a Python list of tuples or `Row` objects."
      ],
      "metadata": {
        "id": "duffJda7IQ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"CreateDFFromList\").getOrCreate()\n",
        "\n",
        "print(\"--- DataFrame from List (Inferred Schema) ---\")\n",
        "# 1. Using a list of tuples with inferred schema (less explicit but quicker for small data)\n",
        "data_inferred = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
        "columns_inferred = [\"Name\", \"ID\"]\n",
        "df_inferred = spark.createDataFrame(data_inferred, columns_inferred)\n",
        "df_inferred.show()\n",
        "df_inferred.printSchema()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "\n",
        "print(\"--- DataFrame from List (Explicit Schema) ---\")\n",
        "# 2. Using a list of rows with explicit schema (recommended for robustness, especially in production)\n",
        "data_explicit = [\n",
        "    (\"David\", 4, \"New York\"),\n",
        "    (\"Eve\", 5, \"London\"),\n",
        "    (\"Frank\", 6, \"Paris\")\n",
        "]\n",
        "\n",
        "schema_explicit = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True)\n",
        "])\n",
        "\n",
        "df_explicit = spark.createDataFrame(data_explicit, schema=schema_explicit)\n",
        "df_explicit.show()\n",
        "df_explicit.printSchema()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OL3hTkQdNPd",
        "outputId": "9c082f15-b526-4c82-aa2e-0efb9945e4dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DataFrame from List (Inferred Schema) ---\n",
            "+-------+---+\n",
            "|   Name| ID|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "+-------+---+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- ID: long (nullable = true)\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- DataFrame from List (Explicit Schema) ---\n",
            "+-----+---+--------+\n",
            "| Name|Age|    City|\n",
            "+-----+---+--------+\n",
            "|David|  4|New York|\n",
            "|  Eve|  5|  London|\n",
            "|Frank|  6|   Paris|\n",
            "+-----+---+--------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Engineer Tip:** While inferred schema is convenient for quick analysis, **explicitly defining schemas for production pipelines is highly recommended.** It prevents unexpected type inference issues, improves performance (Spark doesn't need to sample data to infer types), and acts as documentation.\n",
        "\n",
        "#### B. From RDDs (Python)\n",
        "\n",
        "You can convert a Resilient Distributed Dataset (RDD) to a DataFrame. This is useful when migrating legacy RDD-based logic or integrating with existing RDD data."
      ],
      "metadata": {
        "id": "mh3SskHOWflY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateDFFromRDD\").getOrCreate()\n",
        "sc = spark.sparkContext # SparkContext for RDD operations\n",
        "\n",
        "# Create an RDD of tuples\n",
        "rdd_data = sc.parallelize([\n",
        "    (\"Alice\", 30),\n",
        "    (\"Bob\", 25),\n",
        "    (\"Charlie\", 35)\n",
        "])\n",
        "\n",
        "print(\"--- DataFrame from RDD (Inferred Schema) ---\")\n",
        "# Method 1: Infer schema (less control, provides generic column names if not specified)\n",
        "# When providing a list of strings as schema, it will infer types from the RDD data.\n",
        "df_from_rdd_inferred = spark.createDataFrame(rdd_data, [\"Name\", \"Age\"])\n",
        "df_from_rdd_inferred.show()\n",
        "df_from_rdd_inferred.printSchema()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "\n",
        "print(\"--- DataFrame from RDD (Explicit Schema with Row objects) ---\")\n",
        "# Method 2: Explicitly map to Row objects with a defined schema (recommended for robustness)\n",
        "# Each element in the RDD needs to be a Row object with named fields matching the schema.\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "rdd_rows = rdd_data.map(lambda p: Row(Name=p[0], Age=p[1]))\n",
        "df_from_rdd_explicit = spark.createDataFrame(rdd_rows, schema)\n",
        "df_from_rdd_explicit.show()\n",
        "df_from_rdd_explicit.printSchema()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGWWwqfOWf6d",
        "outputId": "2de52c9c-b375-4c7d-c37e-00dd3fe808b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DataFrame from RDD (Inferred Schema) ---\n",
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- DataFrame from RDD (Explicit Schema with Row objects) ---\n",
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C. From External Files (Python)\n",
        "\n",
        "Spark excels at reading data from various external file formats like CSV, JSON, Parquet, ORC, etc.\n",
        "\n",
        "**To run this example, you'd need sample `people.csv` and `people.json` files:**\n",
        "\n",
        "**`people.csv`:**\n",
        "```csv\n",
        "name,age\n",
        "Alice,30\n",
        "Bob,25\n",
        "Charlie,35\n",
        "```\n",
        "\n",
        "**`people.json`:**\n",
        "```json\n",
        "{\"name\":\"Alice\",\"age\":30}\n",
        "{\"name\":\"Bob\",\"age\":25}\n",
        "{\"name\":\"Charlie\",\"age\":35}\n",
        "```\n"
      ],
      "metadata": {
        "id": "z_cmPVoVWgIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateDFFromFiles\").getOrCreate()\n",
        "\n",
        "# Create dummy files for demonstration (ensure these exist in your working directory)\n",
        "csv_data = \"\"\"name,age\n",
        "Alice,30\n",
        "Bob,25\n",
        "Charlie,35\n",
        "\"\"\"\n",
        "with open(\"people.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "json_data = \"\"\"{\"name\":\"Alice\",\"age\":30}\n",
        "{\"name\":\"Bob\",\"age\":25}\n",
        "{\"name\":\"Charlie\",\"age\":35}\n",
        "\"\"\"\n",
        "with open(\"people.json\", \"w\") as f:\n",
        "    f.write(json_data)\n",
        "\n",
        "\n",
        "# Read CSV file\n",
        "print(\"--- DataFrame from CSV ---\")\n",
        "df_csv = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n",
        "df_csv.show()\n",
        "df_csv.printSchema()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "\n",
        "# Read JSON file\n",
        "print(\"--- DataFrame from JSON ---\")\n",
        "df_json = spark.read.json(\"people.json\")\n",
        "df_json.show()\n",
        "df_json.printSchema()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "\n",
        "# For Parquet, you'd typically write a DataFrame to Parquet first, then read it back\n",
        "# Parquet is a columnar storage format optimized for analytical queries.\n",
        "print(\"--- DataFrame from Parquet (Write then Read) ---\")\n",
        "df_csv.write.mode(\"overwrite\").parquet(\"people.parquet\") # write df_csv to parquet\n",
        "df_parquet = spark.read.parquet(\"people.parquet\") # read from parquet\n",
        "df_parquet.show()\n",
        "df_parquet.printSchema()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUG__-HEWgjf",
        "outputId": "7bf833ac-451e-46f3-fe60-943bb666d283"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DataFrame from CSV ---\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- DataFrame from JSON ---\n",
            "+---+-------+\n",
            "|age|   name|\n",
            "+---+-------+\n",
            "| 30|  Alice|\n",
            "| 25|    Bob|\n",
            "| 35|Charlie|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- DataFrame from Parquet (Write then Read) ---\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Engineer Tip on File Formats:**\n",
        "*   **CSV/JSON:** Human-readable, good for ingestion but less efficient for large-scale analytical queries. `inferSchema=True` can be slow for large files as Spark needs to sample data.\n",
        "*   **Parquet:** Binary, columnar storage format. Highly recommended for intermediate and final data storage in data lakes/warehouses due to:\n",
        "    *   **Columnar compression:** Reduces storage footprint.\n",
        "    *   **Predicate pushdown:** Spark can read only relevant columns and filter data at the source, significantly speeding up queries.\n",
        "    *   **Schema evolution:** Handles changes in schema gracefully.\n",
        "\n",
        "### 2. Basic DataFrame Operations (Actions & Transformations)\n",
        "\n",
        "Once you have a DataFrame, you can perform various fundamental operations.\n",
        "\n",
        "#### A. `select()`\n",
        "\n",
        "`select()` is a **transformation** used to choose specific columns from a DataFrame. It returns a new DataFrame with only the selected columns."
      ],
      "metadata": {
        "id": "0b8hc3AlWijj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRxvsLz7EFrv",
        "outputId": "d248b53b-1769-4556-90ae-813e6caade98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+--------+\n",
            "|   Name|Age|    City|\n",
            "+-------+---+--------+\n",
            "|  Alice| 30|New York|\n",
            "|    Bob| 25|  London|\n",
            "|Charlie| 35|   Paris|\n",
            "+-------+---+--------+\n",
            "\n",
            "--- Select a single column ---\n",
            "+-------+\n",
            "|   Name|\n",
            "+-------+\n",
            "|  Alice|\n",
            "|    Bob|\n",
            "|Charlie|\n",
            "+-------+\n",
            "\n",
            "--- Select multiple columns ---\n",
            "+-------+--------+\n",
            "|   Name|    City|\n",
            "+-------+--------+\n",
            "|  Alice|New York|\n",
            "|    Bob|  London|\n",
            "|Charlie|   Paris|\n",
            "+-------+--------+\n",
            "\n",
            "--- Select columns using col() function (recommended for robustness) ---\n",
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|    Bob| 25|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "--- Select a column and alias it ---\n",
            "+---------+---+\n",
            "|Full_Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 30|\n",
            "|      Bob| 25|\n",
            "|  Charlie| 35|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col # Using 'col' is recommended for robustness\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DFBasicOps\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 30, \"New York\"), (\"Bob\", 25, \"London\"), (\"Charlie\", 35, \"Paris\")]\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "print(\"--- Select a single column ---\")\n",
        "df.select(\"Name\").show()\n",
        "\n",
        "print(\"--- Select multiple columns ---\")\n",
        "df.select(\"Name\", \"City\").show()\n",
        "\n",
        "print(\"--- Select columns using col() function (recommended for robustness) ---\")\n",
        "# Using col() is safer as it prevents ambiguity if a column name clashes with a keyword.\n",
        "df.select(col(\"Name\"), col(\"Age\")).show()\n",
        "\n",
        "print(\"--- Select a column and alias it ---\")\n",
        "df.select(df.Name.alias(\"Full_Name\"), df.Age).show()\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. `filter()` / `where()`\n",
        "\n",
        "`filter()` (or its alias `where()`) is a **transformation** used to filter rows based on a given condition. It returns a new DataFrame containing only the rows that satisfy the condition."
      ],
      "metadata": {
        "id": "Gmq6xTvVWjOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DFBasicOps\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 30, \"New York\"), (\"Bob\", 25, \"London\"), (\"Charlie\", 35, \"Paris\")]\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "print(\"--- Filter using string expression ---\")\n",
        "df.filter(\"Age > 28\").show()\n",
        "\n",
        "print(\"--- Filter using column object (recommended for complex conditions) ---\")\n",
        "df.filter(col(\"Age\") <= 30).show()\n",
        "\n",
        "print(\"--- Filter with multiple conditions (AND) ---\")\n",
        "df.filter((col(\"Age\") > 25) & (col(\"City\") == \"New York\")).show()\n",
        "\n",
        "print(\"--- Filter with multiple conditions (OR) ---\")\n",
        "df.filter((col(\"Age\") < 25) | (col(\"City\") == \"Paris\")).show()\n",
        "\n",
        "print(\"--- Using where() - alias for filter() ---\")\n",
        "df.where(col(\"City\") == \"London\").show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf_Ft85lWjpA",
        "outputId": "69279eb4-700c-4072-fe57-b6a834947ad3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+--------+\n",
            "|   Name|Age|    City|\n",
            "+-------+---+--------+\n",
            "|  Alice| 30|New York|\n",
            "|    Bob| 25|  London|\n",
            "|Charlie| 35|   Paris|\n",
            "+-------+---+--------+\n",
            "\n",
            "--- Filter using string expression ---\n",
            "+-------+---+--------+\n",
            "|   Name|Age|    City|\n",
            "+-------+---+--------+\n",
            "|  Alice| 30|New York|\n",
            "|Charlie| 35|   Paris|\n",
            "+-------+---+--------+\n",
            "\n",
            "--- Filter using column object (recommended for complex conditions) ---\n",
            "+-----+---+--------+\n",
            "| Name|Age|    City|\n",
            "+-----+---+--------+\n",
            "|Alice| 30|New York|\n",
            "|  Bob| 25|  London|\n",
            "+-----+---+--------+\n",
            "\n",
            "--- Filter with multiple conditions (AND) ---\n",
            "+-----+---+--------+\n",
            "| Name|Age|    City|\n",
            "+-----+---+--------+\n",
            "|Alice| 30|New York|\n",
            "+-----+---+--------+\n",
            "\n",
            "--- Filter with multiple conditions (OR) ---\n",
            "+-------+---+-----+\n",
            "|   Name|Age| City|\n",
            "+-------+---+-----+\n",
            "|Charlie| 35|Paris|\n",
            "+-------+---+-----+\n",
            "\n",
            "--- Using where() - alias for filter() ---\n",
            "+----+---+------+\n",
            "|Name|Age|  City|\n",
            "+----+---+------+\n",
            "| Bob| 25|London|\n",
            "+----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C. `show()`\n",
        "\n",
        "`show()` is an **action** that displays the contents of the DataFrame in a tabular format. It's very useful for inspecting your data during development and debugging."
      ],
      "metadata": {
        "id": "DsXXu4KRWkQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DFBasicOps\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 30, \"New York\"), (\"Bob\", 25, \"London\"), (\"Charlie\", 35, \"Paris\")]\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"--- Show entire DataFrame (default 20 rows, truncated) ---\")\n",
        "df.show()\n",
        "\n",
        "print(\"--- Show first N rows ---\")\n",
        "df.show(2) # Shows first 2 rows\n",
        "\n",
        "print(\"--- Show without truncating column values ---\")\n",
        "df.show(truncate=False) # Important for seeing full column values\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gco4HXwlWkls",
        "outputId": "53967c38-ce07-4f79-a418-bcb92e187de1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Show entire DataFrame (default 20 rows, truncated) ---\n",
            "+-------+---+--------+\n",
            "|   Name|Age|    City|\n",
            "+-------+---+--------+\n",
            "|  Alice| 30|New York|\n",
            "|    Bob| 25|  London|\n",
            "|Charlie| 35|   Paris|\n",
            "+-------+---+--------+\n",
            "\n",
            "--- Show first N rows ---\n",
            "+-----+---+--------+\n",
            "| Name|Age|    City|\n",
            "+-----+---+--------+\n",
            "|Alice| 30|New York|\n",
            "|  Bob| 25|  London|\n",
            "+-----+---+--------+\n",
            "only showing top 2 rows\n",
            "\n",
            "--- Show without truncating column values ---\n",
            "+-------+---+--------+\n",
            "|Name   |Age|City    |\n",
            "+-------+---+--------+\n",
            "|Alice  |30 |New York|\n",
            "|Bob    |25 |London  |\n",
            "|Charlie|35 |Paris   |\n",
            "+-------+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D. `printSchema()`\n",
        "\n",
        "`printSchema()` is an **action** that displays the schema (column names and their data types) of the DataFrame in a tree-like format.\n"
      ],
      "metadata": {
        "id": "cMT3IAGyd2QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DFBasicOps\").getOrCreate()\n",
        "\n",
        "# Example with a more complex schema including an ArrayType\n",
        "data = [\n",
        "    (\"Alice\", 30, \"New York\", [\"reading\", \"hiking\"]),\n",
        "    (\"Bob\", 25, \"London\", [\"gaming\"]),\n",
        "    (\"Charlie\", 35, \"Paris\", [])\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"Hobbies\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "print(\"--- DataFrame Schema ---\")\n",
        "df.printSchema() # Crucial for understanding data structure and for debugging type issues.\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ1hT5cndydF",
        "outputId": "c984c663-736e-44bd-a125-a1facc0f3323"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DataFrame Schema ---\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Hobbies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Tip:** `printSchema()` is one of your most valuable tools! Always use it after reading data or performing complex transformations to verify data types and structure. Incorrect data types can lead to errors or performance bottlenecks."
      ],
      "metadata": {
        "id": "NFcQgWm5d6ya"
      }
    }
  ]
}