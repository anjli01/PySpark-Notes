{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/10_Window_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Advanced Analytics: Window Functions\n",
        "\n",
        "Window functions perform calculations across a set of table rows that are somehow related to the current row. Unlike aggregate functions (e.g., `sum()`, `avg()`) which return a single value for an entire group, window functions return a value for *each row*.\n",
        "\n",
        "#### 1.1 Defining a Window Specification (`Window.partitionBy()`, `Window.orderBy()`, `rowsBetween()`)\n",
        "\n",
        "To use a window function, you first define a window specification using the `Window` object.\n",
        "\n",
        "*   `Window.partitionBy(*cols)`:\n",
        "    *   Divides rows into groups (partitions) based on specified columns.\n",
        "    *   The window function is applied independently within each partition.\n",
        "    *   Similar conceptually to `GROUP BY`. If omitted, the entire DataFrame is treated as a single partition.\n",
        "*   `Window.orderBy(*cols)`:\n",
        "    *   Orders the rows *within each partition*.\n",
        "    *   Crucial for functions that depend on row order (e.g., `row_number()`, `rank()`, `lag()`, `lead()`, running totals).\n",
        "*   `rowsBetween(start, end)` / `rangeBetween(start, end)`:\n",
        "    *   Defines the \"frame\" of rows within a partition that the window function operates on.\n",
        "    *   `Window.unboundedPreceding`: From the beginning of the partition.\n",
        "    *   `Window.currentRow`: The current row.\n",
        "    *   `Window.unboundedFollowing`: To the end of the partition.\n",
        "    *   **Common Frames:**\n",
        "        *   `Window.unboundedPreceding, Window.currentRow`: For running sums/averages (includes all rows from start of partition up to current row).\n",
        "        *   `Window.currentRow, Window.unboundedFollowing`: For calculations from the current row to the end.\n",
        "\n",
        "#### 1.2 Common Window Functions\n",
        "\n",
        "| Function          | Description                                                                                             | Notes                                                                                                                                                                                            |\n",
        "| :---------------- | :------------------------------------------------------------------------------------------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `row_number()`    | Assigns a unique, sequential integer to each row within its partition, starting from 1.                 | No ties; each row gets a distinct number.                                                                                                                                                        |\n",
        "| `rank()`          | Assigns a rank to each row within its partition. Ties get the same rank, and a gap is left in the sequence. | If two rows tie for rank 2, the next rank will be 4 (not 3).                                                                                                                                     |\n",
        "| `dense_rank()`    | Similar to `rank()`, but no gaps are left in the ranking sequence when there are ties.                  | If two rows tie for rank 2, the next rank will be 3.                                                                                                                                             |\n",
        "| `lag(col, offset, default_value)` | Returns the value of `col` from a row `offset` rows *before* the current row in the partition. | `offset` (default 1) specifies how many rows back. `default_value` (default None) is used if `offset` goes beyond the partition start (e.g., first row of partition). Useful for previous period comparisons. |\n",
        "| `lead(col, offset, default_value)` | Returns the value of `col` from a row `offset` rows *after* the current row in the partition.  | `offset` (default 1) specifies how many rows forward. `default_value` (default None) is used if `offset` goes beyond the partition end (e.g., last row of partition). Useful for next period comparisons.  |\n",
        "| `sum(col)`        | Calculates the sum of `col` within the defined window frame.                                            | Can be used with `rowsBetween` for running totals.                                                                                                                                               |\n",
        "| `avg(col)`        | Calculates the average of `col` within the defined window frame.                                        | Can be used with `rowsBetween` for moving averages.                                                                                                                                              |\n"
      ],
      "metadata": {
        "id": "tWZZ96s5JbzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number, rank, dense_rank, lag, lead, sum, avg\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"Sales\", \"Alice\", 2023, 100),\n",
        "    (\"Sales\", \"Bob\", 2023, 150),\n",
        "    (\"Sales\", \"Alice\", 2024, 120),\n",
        "    (\"HR\", \"Charlie\", 2023, 80),\n",
        "    (\"HR\", \"David\", 2024, 90),\n",
        "    (\"Sales\", \"Bob\", 2024, 160)\n",
        "]\n",
        "columns = [\"Department\", \"Employee\", \"Year\", \"Sales\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Define a window specification: Partition by Department, order by Year and Sales\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(\"Year\", \"Sales\")\n",
        "\n",
        "# Row Number Example\n",
        "print(\"\\nRow Number by Department and Year/Sales:\")\n",
        "df.withColumn(\"row_num\", row_number().over(window_spec)).show()\n",
        "\n",
        "# Define a window spec for sum of sales per department over all years up to current\n",
        "window_sum = Window.partitionBy(\"Department\").orderBy(\"Year\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Running Sum Example\n",
        "print(\"\\nRunning Sum of Sales per Department:\")\n",
        "df.withColumn(\"Running_Sum_Sales\", sum(\"Sales\").over(window_sum)).show()\n",
        "\n",
        "# --- Ranking functions with ties ---\n",
        "ranking_data = [\n",
        "    (\"DeptA\", \"Alice\", 100),\n",
        "    (\"DeptA\", \"Bob\", 120),\n",
        "    (\"DeptA\", \"Charlie\", 120), # Tie for Bob and Charlie\n",
        "    (\"DeptA\", \"David\", 150),\n",
        "    (\"DeptB\", \"Eve\", 80),\n",
        "    (\"DeptB\", \"Frank\", 80), # Tie for Eve and Frank\n",
        "    (\"DeptB\", \"Grace\", 90)\n",
        "]\n",
        "ranking_cols = [\"Department\", \"Employee\", \"Score\"]\n",
        "ranking_df = spark.createDataFrame(ranking_data, ranking_cols)\n",
        "ranking_df.show()\n",
        "\n",
        "# Define a window specification: Partition by Department, order by Score (descending)\n",
        "window_spec_rank = Window.partitionBy(\"Department\").orderBy(col(\"Score\").desc())\n",
        "\n",
        "print(\"\\nRanking functions with ties:\")\n",
        "ranking_df.withColumn(\"row_num\", row_number().over(window_spec_rank)) \\\n",
        "    .withColumn(\"rank\", rank().over(window_spec_rank)) \\\n",
        "    .withColumn(\"dense_rank\", dense_rank().over(window_spec_rank)) \\\n",
        "    .show()\n",
        "\n",
        "# --- Lag and Lead Example ---\n",
        "sales_data = [\n",
        "    (\"Alice\", 2022, 1000),\n",
        "    (\"Alice\", 2023, 1200),\n",
        "    (\"Alice\", 2024, 1100),\n",
        "    (\"Bob\", 2022, 1500),\n",
        "    (\"Bob\", 2023, 1600)\n",
        "]\n",
        "sales_cols = [\"Employee\", \"Year\", \"Sales\"]\n",
        "sales_df = spark.createDataFrame(sales_data, sales_cols)\n",
        "sales_df.show()\n",
        "\n",
        "# Window for lag/lead: partition by Employee, order by Year\n",
        "window_lag_lead = Window.partitionBy(\"Employee\").orderBy(\"Year\")\n",
        "\n",
        "print(\"\\nLag and Lead for Sales:\")\n",
        "sales_df.withColumn(\"Prev_Year_Sales\", lag(\"Sales\", 1).over(window_lag_lead)) \\\n",
        "    .withColumn(\"Next_Year_Sales\", lead(\"Sales\", 1).over(window_lag_lead)) \\\n",
        "    .show()\n",
        "\n",
        "# spark.stop() # Uncomment if this is the end of your script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiOYEPFLjoZX",
        "outputId": "a805f53b-c42d-40f3-f985-2e2f7442677b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----+-----+\n",
            "|Department|Employee|Year|Sales|\n",
            "+----------+--------+----+-----+\n",
            "|     Sales|   Alice|2023|  100|\n",
            "|     Sales|     Bob|2023|  150|\n",
            "|     Sales|   Alice|2024|  120|\n",
            "|        HR| Charlie|2023|   80|\n",
            "|        HR|   David|2024|   90|\n",
            "|     Sales|     Bob|2024|  160|\n",
            "+----------+--------+----+-----+\n",
            "\n",
            "\n",
            "Row Number by Department and Year/Sales:\n",
            "+----------+--------+----+-----+-------+\n",
            "|Department|Employee|Year|Sales|row_num|\n",
            "+----------+--------+----+-----+-------+\n",
            "|        HR| Charlie|2023|   80|      1|\n",
            "|        HR|   David|2024|   90|      2|\n",
            "|     Sales|   Alice|2023|  100|      1|\n",
            "|     Sales|     Bob|2023|  150|      2|\n",
            "|     Sales|   Alice|2024|  120|      3|\n",
            "|     Sales|     Bob|2024|  160|      4|\n",
            "+----------+--------+----+-----+-------+\n",
            "\n",
            "\n",
            "Running Sum of Sales per Department:\n",
            "+----------+--------+----+-----+-----------------+\n",
            "|Department|Employee|Year|Sales|Running_Sum_Sales|\n",
            "+----------+--------+----+-----+-----------------+\n",
            "|        HR| Charlie|2023|   80|               80|\n",
            "|        HR|   David|2024|   90|              170|\n",
            "|     Sales|   Alice|2023|  100|              100|\n",
            "|     Sales|     Bob|2023|  150|              250|\n",
            "|     Sales|   Alice|2024|  120|              370|\n",
            "|     Sales|     Bob|2024|  160|              530|\n",
            "+----------+--------+----+-----+-----------------+\n",
            "\n",
            "+----------+--------+-----+\n",
            "|Department|Employee|Score|\n",
            "+----------+--------+-----+\n",
            "|     DeptA|   Alice|  100|\n",
            "|     DeptA|     Bob|  120|\n",
            "|     DeptA| Charlie|  120|\n",
            "|     DeptA|   David|  150|\n",
            "|     DeptB|     Eve|   80|\n",
            "|     DeptB|   Frank|   80|\n",
            "|     DeptB|   Grace|   90|\n",
            "+----------+--------+-----+\n",
            "\n",
            "\n",
            "Ranking functions with ties:\n",
            "+----------+--------+-----+-------+----+----------+\n",
            "|Department|Employee|Score|row_num|rank|dense_rank|\n",
            "+----------+--------+-----+-------+----+----------+\n",
            "|     DeptA|   David|  150|      1|   1|         1|\n",
            "|     DeptA|     Bob|  120|      2|   2|         2|\n",
            "|     DeptA| Charlie|  120|      3|   2|         2|\n",
            "|     DeptA|   Alice|  100|      4|   4|         3|\n",
            "|     DeptB|   Grace|   90|      1|   1|         1|\n",
            "|     DeptB|     Eve|   80|      2|   2|         2|\n",
            "|     DeptB|   Frank|   80|      3|   2|         2|\n",
            "+----------+--------+-----+-------+----+----------+\n",
            "\n",
            "+--------+----+-----+\n",
            "|Employee|Year|Sales|\n",
            "+--------+----+-----+\n",
            "|   Alice|2022| 1000|\n",
            "|   Alice|2023| 1200|\n",
            "|   Alice|2024| 1100|\n",
            "|     Bob|2022| 1500|\n",
            "|     Bob|2023| 1600|\n",
            "+--------+----+-----+\n",
            "\n",
            "\n",
            "Lag and Lead for Sales:\n",
            "+--------+----+-----+---------------+---------------+\n",
            "|Employee|Year|Sales|Prev_Year_Sales|Next_Year_Sales|\n",
            "+--------+----+-----+---------------+---------------+\n",
            "|   Alice|2022| 1000|           NULL|           1200|\n",
            "|   Alice|2023| 1200|           1000|           1100|\n",
            "|   Alice|2024| 1100|           1200|           NULL|\n",
            "|     Bob|2022| 1500|           NULL|           1600|\n",
            "|     Bob|2023| 1600|           1500|           NULL|\n",
            "+--------+----+-----+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Practical Use Cases for Window Functions\n",
        "\n",
        "Window functions are incredibly versatile for various data engineering tasks.\n",
        "\n",
        "*   **De-duplication:**\n",
        "    *   Find duplicate rows based on a subset of columns.\n",
        "    *   Assign `row_number()` or `rank()` to each duplicate group within partitions.\n",
        "    *   Filter to keep only the first (or last) occurrence.\n",
        "    *   **Process:**\n",
        "        1.  `Window.partitionBy(\"unique_key_columns\")`: Group rows that are considered duplicates.\n",
        "        2.  `Window.orderBy(\"criteria_for_keeping_one\")`: Decide which duplicate to keep (e.g., latest timestamp, highest ID).\n",
        "        3.  Apply `row_number().over(...)`.\n",
        "        4.  `filter(col(\"row_num\") == 1)` to keep only the first (or desired) record in each partition."
      ],
      "metadata": {
        "id": "D42sWeKiXMpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM2WXLbFEA_j",
        "outputId": "2799dca0-a5de-4c2e-8f75-173618e21da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame with duplicates:\n",
            "+-------+-----------------+----+--------+\n",
            "|   Name|            Email|City|RecordID|\n",
            "+-------+-----------------+----+--------+\n",
            "|  Alice|  alice@email.com|  NY|       1|\n",
            "|    Bob|    bob@email.com|  LD|       2|\n",
            "|  Alice|  alice@email.com|  LA|       3|\n",
            "|Charlie|charlie@email.com|  SF|       4|\n",
            "|    Bob|    bob@email.com|  LD|       5|\n",
            "+-------+-----------------+----+--------+\n",
            "\n",
            "\n",
            "Deduplicated DataFrame (keeping latest record):\n",
            "+-------+-----------------+----+--------+\n",
            "|   Name|            Email|City|RecordID|\n",
            "+-------+-----------------+----+--------+\n",
            "|  Alice|  alice@email.com|  LA|       3|\n",
            "|    Bob|    bob@email.com|  LD|       5|\n",
            "|Charlie|charlie@email.com|  SF|       4|\n",
            "+-------+-----------------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Deduplication\").getOrCreate()\n",
        "\n",
        "duplicate_data = [\n",
        "    (\"Alice\", \"alice@email.com\", \"NY\", 1),\n",
        "    (\"Bob\", \"bob@email.com\", \"LD\", 2),\n",
        "    (\"Alice\", \"alice@email.com\", \"LA\", 3), # Duplicate Alice, different city\n",
        "    (\"Charlie\", \"charlie@email.com\", \"SF\", 4),\n",
        "    (\"Bob\", \"bob@email.com\", \"LD\", 5)  # Exact duplicate Bob, higher RecordID\n",
        "]\n",
        "dup_cols = [\"Name\", \"Email\", \"City\", \"RecordID\"]\n",
        "dup_df = spark.createDataFrame(duplicate_data, dup_cols)\n",
        "print(\"Original DataFrame with duplicates:\")\n",
        "dup_df.show()\n",
        "\n",
        "# Deduplicate keeping the record with the higher RecordID (more recent)\n",
        "window_spec_dedup = Window.partitionBy(\"Name\", \"Email\").orderBy(col(\"RecordID\").desc())\n",
        "\n",
        "deduplicated_df = dup_df.withColumn(\"row_num\", row_number().over(window_spec_dedup)) \\\n",
        "    .filter(col(\"row_num\") == 1) \\\n",
        "    .drop(\"row_num\")\n",
        "\n",
        "print(\"\\nDeduplicated DataFrame (keeping latest record):\")\n",
        "deduplicated_df.show()\n",
        "\n",
        "# spark.stop() # Uncomment if this is the end of your script"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Ranking:**\n",
        "    *   Assign ranks to items within categories (e.g., top 3 students per class, top 10 products per region).\n",
        "    *   Use `rank()`, `dense_rank()`, or `row_number()` based on tie-breaking requirements.\n",
        "\n",
        "*   **Time Series Logic:**\n",
        "    *   **Moving Averages/Cumulative Sums:** Use `rowsBetween(Window.unboundedPreceding, Window.currentRow)` or specific N-row windows.\n",
        "    *   **Period-over-period comparison:** Compare current values with previous or subsequent values using `lag()` and `lead()`."
      ],
      "metadata": {
        "id": "DeWnzogLXNF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, sum\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TimeSeries\").getOrCreate()\n",
        "\n",
        "time_series_data = [\n",
        "    (\"ProductA\", \"2023-01-01\", 100),\n",
        "    (\"ProductA\", \"2023-02-01\", 120),\n",
        "    (\"ProductA\", \"2023-03-01\", 110),\n",
        "    (\"ProductB\", \"2023-01-01\", 200),\n",
        "    (\"ProductB\", \"2023-02-01\", 230)\n",
        "]\n",
        "ts_cols = [\"Product\", \"Date\", \"Sales\"]\n",
        "ts_df = spark.createDataFrame(time_series_data, ts_cols)\n",
        "print(\"Original Time Series DataFrame:\")\n",
        "ts_df.show()\n",
        "\n",
        "window_ts = Window.partitionBy(\"Product\").orderBy(\"Date\")\n",
        "\n",
        "# Calculate previous month's sales and month-over-month growth\n",
        "# Note: For MoM Growth, typically handle division by zero or first period specially.\n",
        "ts_df_with_growth = ts_df.withColumn(\"PreviousMonthSales\", lag(\"Sales\", 1).over(window_ts)) \\\n",
        "    .withColumn(\"MoM_Growth\",\n",
        "                (col(\"Sales\") - col(\"PreviousMonthSales\")) / col(\"PreviousMonthSales\"))\n",
        "\n",
        "print(\"\\nTime Series with Previous Month Sales and MoM Growth:\")\n",
        "ts_df_with_growth.show()\n",
        "\n",
        "# Calculate cumulative sum of sales for each product\n",
        "ts_df_cumulative = ts_df.withColumn(\"CumulativeSales\",\n",
        "    sum(\"Sales\").over(window_ts.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
        "\n",
        "print(\"\\nTime Series with Cumulative Sales:\")\n",
        "ts_df_cumulative.show()\n",
        "\n",
        "# spark.stop() # Uncomment if this is the end of your script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf-kmmYyXNWj",
        "outputId": "369630df-bd40-42cf-99b2-900843561c07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Time Series DataFrame:\n",
            "+--------+----------+-----+\n",
            "| Product|      Date|Sales|\n",
            "+--------+----------+-----+\n",
            "|ProductA|2023-01-01|  100|\n",
            "|ProductA|2023-02-01|  120|\n",
            "|ProductA|2023-03-01|  110|\n",
            "|ProductB|2023-01-01|  200|\n",
            "|ProductB|2023-02-01|  230|\n",
            "+--------+----------+-----+\n",
            "\n",
            "\n",
            "Time Series with Previous Month Sales and MoM Growth:\n",
            "+--------+----------+-----+------------------+--------------------+\n",
            "| Product|      Date|Sales|PreviousMonthSales|          MoM_Growth|\n",
            "+--------+----------+-----+------------------+--------------------+\n",
            "|ProductA|2023-01-01|  100|              NULL|                NULL|\n",
            "|ProductA|2023-02-01|  120|               100|                 0.2|\n",
            "|ProductA|2023-03-01|  110|               120|-0.08333333333333333|\n",
            "|ProductB|2023-01-01|  200|              NULL|                NULL|\n",
            "|ProductB|2023-02-01|  230|               200|                0.15|\n",
            "+--------+----------+-----+------------------+--------------------+\n",
            "\n",
            "\n",
            "Time Series with Cumulative Sales:\n",
            "+--------+----------+-----+---------------+\n",
            "| Product|      Date|Sales|CumulativeSales|\n",
            "+--------+----------+-----+---------------+\n",
            "|ProductA|2023-01-01|  100|            100|\n",
            "|ProductA|2023-02-01|  120|            220|\n",
            "|ProductA|2023-03-01|  110|            330|\n",
            "|ProductB|2023-01-01|  200|            200|\n",
            "|ProductB|2023-02-01|  230|            430|\n",
            "+--------+----------+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}