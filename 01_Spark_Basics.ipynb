{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/01_Spark_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Spark Basics\n",
        "\n",
        "*   **What is Spark?**\n",
        "    *   A unified analytics engine for large-scale data processing.\n",
        "    *   Significantly faster than traditional Hadoop MapReduce for many workloads.\n",
        "    *   Provides high-level APIs in Scala, Java, Python, and R.\n",
        "    *   Designed for batch processing, interactive queries, streaming, and machine learning.\n",
        "\n",
        "### 2. Spark Entry Points: `SparkSession` & `SparkContext`\n",
        "\n",
        "To interact with Spark, you need to establish an entry point.\n",
        "\n",
        "*   **`SparkContext` (Legacy):**\n",
        "    *   The original entry point for Spark 1.x.\n",
        "    *   Responsible for connecting to the Spark cluster and managing resources.\n",
        "    *   Primarily used for RDD operations.\n",
        "*   **`SparkSession` (Modern & Recommended):**\n",
        "    *   The unified entry point for Spark 2.x and later.\n",
        "    *   Combines the functionalities of `SparkContext`, `SQLContext`, `HiveContext`, and `StreamingContext` into a single object.\n",
        "    *   Recommended for all new Spark applications, as it provides access to all Spark features (RDDs, DataFrames, Datasets, SQL, Streaming).\n",
        "    *   **`SparkSession.builder.getOrCreate()`**:\n",
        "        *   This method is the standard way to initialize `SparkSession`.\n",
        "        *   `builder`: Returns a `SparkSession.Builder` object for configuring Spark properties.\n",
        "        *   `getOrCreate()`:\n",
        "            *   If a `SparkSession` instance already exists, it returns the existing one.\n",
        "            *   If not, it creates a new one based on the builder's configuration.\n",
        "        *   This ensures you always have a single, active `SparkSession` per application.\n",
        "\n",
        "**Example (Python): Initializing SparkSession**\n"
      ],
      "metadata": {
        "id": "-45Bz1iMExzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48A4bdihD3W1",
        "outputId": "05820c32-dfa0-4fd3-a87d-effdf0e41e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession created: 3.5.1\n",
            "SparkContext created: 3.5.1\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "# appName: A name for your application, shown in the Spark UI.\n",
        "# master: 'local' runs Spark locally on your machine using as many cores as specified ('local[*]' uses all available cores).\n",
        "# For a cluster, this would be a URL like 'spark://host:port' or 'yarn'.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApplication\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Access SparkContext via SparkSession (for RDD operations if needed)\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"SparkSession created: {spark.version}\")\n",
        "print(f\"SparkContext created: {sc.version}\")\n",
        "\n",
        "# Stop the SparkSession when done\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Spark Data Structures: RDD, DataFrame, and Dataset\n",
        "\n",
        "Spark offers three primary data structures, each with distinct characteristics and use cases. Understanding their differences is crucial for efficient Spark development.\n",
        "\n",
        "**Comparison Table:**\n",
        "\n",
        "| Feature            | RDD (Resilient Distributed Dataset)                                 | DataFrame                                                                     | Dataset (Scala/Java only)                                                     |\n",
        "| :----------------- | :------------------------------------------------------------------ | :---------------------------------------------------------------------------- | :---------------------------------------------------------------------------- |\n",
        "| **Abstraction Level** | Low-level, object-oriented API                                      | High-level, tabular abstraction (named columns)                               | High-level, strongly-typed objects (JVM)                                      |\n",
        "| **Data Structure** | Distributed collection of objects                                   | Distributed collection of Row objects with schema                             | Distributed collection of Scala case classes or Java beans with schema        |\n",
        "| **Type Safety**    | No compile-time type safety (runtime errors possible)               | No compile-time type safety (runtime errors possible if column names are wrong) | **Compile-time type safety**                                                  |\n",
        "| **Optimization**   | **Less optimized** (Spark doesn't know schema, limited internal optimization) | **Highly optimized** (Leverages Catalyst Optimizer & Tungsten)              | **Highly optimized** (Leverages Catalyst Optimizer & Tungsten)              |\n",
        "| **API**            | Lambda functions, map, filter, reduce                               | SQL-like API, operations on named columns                                     | Functional API on objects, operations on fields of case classes               |\n",
        "| **Language Support** | Scala, Java, Python, R                                              | Scala, Java, Python, R                                                        | **Scala, Java only**                                                          |\n",
        "| **Use Cases**      | Unstructured data, custom transformations, when fine-grained control is needed | Structured & semi-structured data, most common use cases, SQL queries         | Structured data where compile-time type safety is critical (Scala/Java)       |\n",
        "\n",
        "#### 3.1. RDD: Resilient Distributed Dataset\n",
        "\n",
        "*   **Definition:** The fundamental data structure of Spark. A fault-tolerant, immutable, distributed collection of objects that can be operated on in parallel.\n",
        "*   **Key Characteristics:**\n",
        "    *   **Low-level & Object-oriented:** Provides granular control, working directly with Python/Scala/Java objects.\n",
        "    *   **Immutable:** Once created, you cannot change an RDD. Transformations create new RDDs.\n",
        "    *   **Fault-tolerant:** Can automatically recover lost data partitions.\n",
        "    *   **Distributed:** Data is partitioned across nodes in a cluster.\n",
        "    *   **Less Optimized:** Spark's internal optimizers have limited visibility into the data's internal structure because RDDs don't have a schema. This means Spark can't perform many performance optimizations automatically; the developer is responsible.\n",
        "*   **When to use:** When you need very fine-grained control over your data transformations, are dealing with truly unstructured data where schema inference is not possible, or when you are implementing custom Spark functionalities. For most modern use cases, DataFrames are preferred.\n",
        "\n",
        "**Example (Python): RDD Operations**\n"
      ],
      "metadata": {
        "id": "zxE_0E0TE-Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "# 'local' means running Spark in local mode. \"RDD_Example\" is the application name.\n",
        "sc = SparkContext(\"local\", \"RDD_Example\")\n",
        "\n",
        "# Create an RDD from a Python list\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Perform a transformation: Multiply each element by 2\n",
        "rdd_transformed = rdd.map(lambda x: x * 2)\n",
        "\n",
        "# Collect and print the results (collect() brings data from distributed RDD to local driver)\n",
        "print(\"RDD transformed data:\", rdd_transformed.collect()) # Expected: [2, 4, 6, 8, 10]\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBaiCMWCE2Z3",
        "outputId": "1ec79a0c-0a2d-4830-c59f-249d9530c8d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD transformed data: [2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3.2. DataFrame\n",
        "\n",
        "*   **Definition:** A distributed collection of data organized into named columns, conceptually equivalent to a table in a relational database or a Pandas DataFrame.\n",
        "*   **Key Characteristics:**\n",
        "    *   **Schema-aware:** Has a well-defined schema (column names and types), which is crucial for Spark to understand and optimize data operations.\n",
        "    *   **Optimized with Catalyst Optimizer:** This is the primary advantage. Spark's Catalyst Optimizer uses the schema information to build an optimized logical and physical plan for queries.\n",
        "        *   **Rule-based optimization:** Applies rules (e.g., predicate pushdown, column pruning) to simplify and optimize the query plan.\n",
        "        *   **Cost-based optimization:** Uses data statistics to choose the most efficient physical execution plan.\n",
        "    *   **Tungsten Execution Engine:** DataFrames leverage Tungsten, which performs operations directly on serialized binary data in memory, reducing memory overhead and improving CPU utilization.\n",
        "        *   **Memory Management:** Optimized allocation and deallocation.\n",
        "        *   **Code Generation:** Generates efficient bytecode for operations.\n",
        "        *   **Cache Locality:** Improves data access patterns.\n",
        "    *   **Ease of Use & Language Agnostic:** Offers a high-level, SQL-like API that's intuitive and available across all major Spark languages (Scala, Java, Python, R).\n",
        "*   **When to use:** For most Spark workloads involving structured or semi-structured data. DataFrames are generally the preferred choice due to their balance of ease of use and performance optimizations.\n",
        "\n",
        "**Example (Python): DataFrame Operations**\n"
      ],
      "metadata": {
        "id": "rqZxa2VvFGJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession (using getOrCreate ensures only one session is active)\n",
        "spark = SparkSession.builder \\\n",
        "                    .appName(\"DataFrame_Example\") \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "# Create a DataFrame from a list of tuples with schema\n",
        "data = [(\"Alice\", 1),\n",
        "        (\"Bob\", 2),\n",
        "        (\"Charlie\", 3),\n",
        "        (\"David\", 4)]\n",
        "columns = [\"Name\", \"ID\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8kXMiXkFBtA",
        "outputId": "7284530b-129b-45fa-e695-3cb9bc5f36c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---+\n",
            "|   Name| ID|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "|  David|  4|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDataFrame Schema:\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv7jTigpZsYA",
        "outputId": "28883286-d397-4bac-fdb7-17f31b0bbce8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame Schema:\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- ID: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a transformation: Filter rows where ID > 1\n",
        "df_filtered = df.filter(col(\"ID\") > 1) # Using 'col' for clarity and safety, can also use df.ID > 1\n",
        "\n",
        "print(\"\\nFiltered DataFrame (ID > 1):\")\n",
        "df_filtered.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXkaDuHoZsI-",
        "outputId": "67d9dd8b-8d53-46d5-d372-8aed79df3357"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtered DataFrame (ID > 1):\n",
            "+-------+---+\n",
            "|   Name| ID|\n",
            "+-------+---+\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "|  David|  4|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. Dataset (Scala/Java only)\n",
        "\n",
        "*   **Definition:** A strongly-typed collection of JVM objects that combines the best features of RDDs (compile-time type safety) and DataFrames (optimization with Catalyst).\n",
        "*   **Key Characteristics:**\n",
        "    *   **Strongly Typed:** Provides compile-time type safety. If you try to access a non-existent column or use an incompatible type, you'll get an error at compilation time, not runtime.\n",
        "    *   **Optimized with Catalyst & Tungsten:** Benefits from the same performance optimizations as DataFrames.\n",
        "    *   **Encoders:** Use encoders to efficiently serialize and deserialize JVM objects to/from Spark's internal Tungsten binary format, optimizing storage and processing.\n",
        "*   **Limitations:** Exclusively available in Scala and Java. Python and R do not support Datasets directly, as they are not statically typed JVM languages.\n",
        "*   **When to use:** When working in Scala or Java and type safety is paramount, providing peace of mind during development and preventing runtime errors related to schema mismatches.\n",
        "\n",
        "**Conceptual Example (Scala - illustrating the idea):**\n",
        "\n",
        "```scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "// Initialize SparkSession\n",
        "val spark = SparkSession.builder()\n",
        "  .appName(\"Dataset_Example\")\n",
        "  .master(\"local[*]\")\n",
        "  .getOrCreate()\n",
        "\n",
        "// Define a case class (strongly-typed object)\n",
        "case class Person(name: String, age: Long)\n",
        "\n",
        "// Create a DataFrame (e.g., from JSON, schema inferred)\n",
        "val peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
        "// In a real scenario, \"people.json\" would contain data like: {\"name\":\"Alice\",\"age\":30}, {\"name\":\"Bob\",\"age\":25}\n",
        "\n",
        "// Convert the DataFrame to a Dataset of Person objects by providing the case class\n",
        "// This is where the strong typing comes in. Spark will try to map DataFrame columns to Person fields.\n",
        "val peopleDS = peopleDF.as[Person]\n",
        "\n",
        "// Perform a transformation using object-oriented API (with compile-time type safety)\n",
        "// `_.age` directly accesses the 'age' field of the `Person` object.\n",
        "val filteredPeopleDS = peopleDS.filter(_.age > 25)\n",
        "\n",
        "filteredPeopleDS.show()\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "### 4. Why DataFrame is Generally Preferred\n",
        "\n",
        "For most Spark workloads, DataFrames are the go-to choice due to a compelling combination of factors:\n",
        "\n",
        "*   **Ease of Use:** DataFrames offer a high-level, SQL-like API that is intuitive for anyone familiar with relational databases or data manipulation libraries like Pandas. This abstracts away much of the complexity of distributed processing.\n",
        "*   **Optimized Performance:** The built-in **Catalyst Optimizer** and **Tungsten Execution Engine** automatically optimize queries for significantly better performance compared to RDDs, without requiring manual optimization by the developer.\n",
        "*   **Broad Language Support:** DataFrames are equally powerful and available across all major Spark-supported languages (Python, Scala, Java, R), making them versatile for diverse teams.\n",
        "*   **Schema Awareness:** Having a defined schema allows Spark to perform efficient operations, detect errors earlier, and provide better tooling."
      ],
      "metadata": {
        "id": "Ch8IqIE7FQf2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZ1sz1J-aCpF"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}