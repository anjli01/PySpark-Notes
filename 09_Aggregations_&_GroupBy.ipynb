{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/09_Aggregations_%26_GroupBy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Aggregations & GroupBy\n",
        "\n",
        "Aggregations are fundamental operations to summarize data, typically performed on groups of rows. The `groupBy()` function is used to define these groups.\n",
        "\n",
        "### Core Concepts & Functions:\n",
        "\n",
        "*   **`groupBy(*cols)`**: Groups a DataFrame by one or more specified columns. It returns a `GroupedData` object, on which aggregation functions can be applied. If no `groupBy()` is used, aggregations apply to the entire DataFrame.\n",
        "*   **`agg(*exprs)`**: The most flexible way to apply one or more aggregation functions to grouped data (or the entire DataFrame). You can pass standard aggregate functions (e.g., `count()`, `sum()`) or define custom ones.\n",
        "    *   **Renaming Output Columns**: Always use `.alias(\"new_column_name\")` with aggregation expressions within `agg()` for clear and meaningful output column names.\n",
        "*   **Direct Aggregation Functions**: These are shorthand methods directly available on a `GroupedData` object for common aggregations:\n",
        "    *   `count()`: Returns the number of items in each group.\n",
        "    *   `sum(col)`: Calculates the sum of values in a numeric column for each group.\n",
        "    *   `avg(col)`: Computes the average of values in a numeric column for each group.\n",
        "    *   `min(col)`: Finds the minimum value in a column for each group.\n",
        "    *   `max(col)`: Finds the maximum value in a column for each group.\n",
        "*   **Source**: All aggregate functions (like `count`, `sum`, `avg`, `min`, `max`) are available in `pyspark.sql.functions`.\n",
        "\n",
        "### Why it's important for Data Engineers:\n",
        "\n",
        "*   **Data Summarization**: Crucial for generating reports, dashboards, and key performance indicators (KPIs).\n",
        "*   **Feature Engineering**: Aggregations are often used to create new features for machine learning models (e.g., total sales per customer, average transaction value).\n",
        "*   **Data Validation**: Quickly verify data integrity by checking counts, sums, or averages.\n",
        "\n",
        "### Example (Python):"
      ],
      "metadata": {
        "id": "PD8ttqxqJZYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, sum, avg, min, max\n",
        "\n",
        "# 1. Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Aggregations\").getOrCreate()\n",
        "\n",
        "# 2. Create Sample Data\n",
        "data = [(\"A\", \"Sales\", 1000),\n",
        "        (\"B\", \"Sales\", 1500),\n",
        "        (\"A\", \"HR\", 800),\n",
        "        (\"C\", \"IT\", 2000),\n",
        "        (\"B\", \"HR\", 1200),\n",
        "        (\"C\", \"Sales\", 1800)]\n",
        "columns = [\"Employee\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# --- Single Aggregations ---\n",
        "\n",
        "print(\"\\n--- Count of employees per Department ---\")\n",
        "df.groupBy(\"Department\").count().show()\n",
        "\n",
        "print(\"\\n--- Sum of salary per Department ---\")\n",
        "df.groupBy(\"Department\").sum(\"Salary\").show()\n",
        "\n",
        "print(\"\\n--- Average salary per Department ---\")\n",
        "df.groupBy(\"Department\").avg(\"Salary\").show()\n",
        "\n",
        "print(\"\\n--- Min and Max salary per Department ---\")\n",
        "# Note: min and max can be chained, but it's clearer with .agg() for multiple\n",
        "df.groupBy(\"Department\").min(\"Salary\").show() # Shows only min\n",
        "df.groupBy(\"Department\").max(\"Salary\").show() # Shows only max\n",
        "\n",
        "\n",
        "print(\"\\n--- Using agg() for single aggregation (equivalent to direct agg function) ---\")\n",
        "df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"TotalSalary\")).show()\n",
        "\n",
        "# --- Aggregating over multiple columns ---\n",
        "\n",
        "data_multi = [(\"A\", \"Sales\", \"NY\", 1000),\n",
        "              (\"B\", \"Sales\", \"LD\", 1500),\n",
        "              (\"A\", \"HR\", \"NY\", 800),\n",
        "              (\"C\", \"IT\", \"SF\", 2000),\n",
        "              (\"B\", \"HR\", \"LD\", 1200),\n",
        "              (\"C\", \"Sales\", \"NY\", 1800),\n",
        "              (\"A\", \"Sales\", \"NY\", 900)]\n",
        "columns_multi = [\"Employee\", \"Department\", \"City\", \"Salary\"]\n",
        "df_multi = spark.createDataFrame(data_multi, columns_multi)\n",
        "df_multi.show()\n",
        "\n",
        "print(\"\\n--- Count of employees per Department and City ---\")\n",
        "df_multi.groupBy(\"Department\", \"City\").count().show()\n",
        "\n",
        "print(\"\\n--- Sum and average of salary per Department and City ---\")\n",
        "df_multi.groupBy(\"Department\", \"City\") \\\n",
        "    .agg(sum(\"Salary\").alias(\"TotalSalary\"),\n",
        "         avg(\"Salary\").alias(\"AverageSalary\")) \\\n",
        "    .show()\n",
        "\n",
        "# --- Aggregating on entire DataFrame (no groupBy) ---\n",
        "\n",
        "print(\"\\n--- Aggregations on entire DataFrame ---\")\n",
        "df.agg(count(\"Employee\").alias(\"TotalEmployees\"),\n",
        "       sum(\"Salary\").alias(\"GrandTotalSalary\"),\n",
        "       avg(\"Salary\").alias(\"OverallAverageSalary\")).show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_SocOJYjD1a",
        "outputId": "1c910941-fa8a-4a16-e9a9-148b3c9b82fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|Employee|Department|Salary|\n",
            "+--------+----------+------+\n",
            "|       A|     Sales|  1000|\n",
            "|       B|     Sales|  1500|\n",
            "|       A|        HR|   800|\n",
            "|       C|        IT|  2000|\n",
            "|       B|        HR|  1200|\n",
            "|       C|     Sales|  1800|\n",
            "+--------+----------+------+\n",
            "\n",
            "\n",
            "--- Count of employees per Department ---\n",
            "+----------+-----+\n",
            "|Department|count|\n",
            "+----------+-----+\n",
            "|     Sales|    3|\n",
            "|        HR|    2|\n",
            "|        IT|    1|\n",
            "+----------+-----+\n",
            "\n",
            "\n",
            "--- Sum of salary per Department ---\n",
            "+----------+-----------+\n",
            "|Department|sum(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       4300|\n",
            "|        HR|       2000|\n",
            "|        IT|       2000|\n",
            "+----------+-----------+\n",
            "\n",
            "\n",
            "--- Average salary per Department ---\n",
            "+----------+------------------+\n",
            "|Department|       avg(Salary)|\n",
            "+----------+------------------+\n",
            "|     Sales|1433.3333333333333|\n",
            "|        HR|            1000.0|\n",
            "|        IT|            2000.0|\n",
            "+----------+------------------+\n",
            "\n",
            "\n",
            "--- Min and Max salary per Department ---\n",
            "+----------+-----------+\n",
            "|Department|min(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       1000|\n",
            "|        HR|        800|\n",
            "|        IT|       2000|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|max(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       1800|\n",
            "|        HR|       1200|\n",
            "|        IT|       2000|\n",
            "+----------+-----------+\n",
            "\n",
            "\n",
            "--- Using agg() for single aggregation (equivalent to direct agg function) ---\n",
            "+----------+-----------+\n",
            "|Department|TotalSalary|\n",
            "+----------+-----------+\n",
            "|     Sales|       4300|\n",
            "|        HR|       2000|\n",
            "|        IT|       2000|\n",
            "+----------+-----------+\n",
            "\n",
            "+--------+----------+----+------+\n",
            "|Employee|Department|City|Salary|\n",
            "+--------+----------+----+------+\n",
            "|       A|     Sales|  NY|  1000|\n",
            "|       B|     Sales|  LD|  1500|\n",
            "|       A|        HR|  NY|   800|\n",
            "|       C|        IT|  SF|  2000|\n",
            "|       B|        HR|  LD|  1200|\n",
            "|       C|     Sales|  NY|  1800|\n",
            "|       A|     Sales|  NY|   900|\n",
            "+--------+----------+----+------+\n",
            "\n",
            "\n",
            "--- Count of employees per Department and City ---\n",
            "+----------+----+-----+\n",
            "|Department|City|count|\n",
            "+----------+----+-----+\n",
            "|        HR|  NY|    1|\n",
            "|     Sales|  LD|    1|\n",
            "|     Sales|  NY|    3|\n",
            "|        HR|  LD|    1|\n",
            "|        IT|  SF|    1|\n",
            "+----------+----+-----+\n",
            "\n",
            "\n",
            "--- Sum and average of salary per Department and City ---\n",
            "+----------+----+-----------+------------------+\n",
            "|Department|City|TotalSalary|     AverageSalary|\n",
            "+----------+----+-----------+------------------+\n",
            "|        HR|  NY|        800|             800.0|\n",
            "|     Sales|  LD|       1500|            1500.0|\n",
            "|     Sales|  NY|       3700|1233.3333333333333|\n",
            "|        HR|  LD|       1200|            1200.0|\n",
            "|        IT|  SF|       2000|            2000.0|\n",
            "+----------+----+-----------+------------------+\n",
            "\n",
            "\n",
            "--- Aggregations on entire DataFrame ---\n",
            "+--------------+----------------+--------------------+\n",
            "|TotalEmployees|GrandTotalSalary|OverallAverageSalary|\n",
            "+--------------+----------------+--------------------+\n",
            "|             6|            8300|  1383.3333333333333|\n",
            "+--------------+----------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Pivot Operations\n",
        "\n",
        "Pivot operations (also known as cross-tabulations) transform data by turning unique values from one column into new columns. This is useful for reshaping data from a \"long\" format to a \"wide\" format.\n",
        "\n",
        "### Steps for Pivoting:\n",
        "\n",
        "1.  **`groupBy(*cols)`**: Group the DataFrame by the column(s) that will remain as rows in the pivoted output.\n",
        "2.  **`pivot(pivot_column, [values])`**: Specify the column whose unique values will become new columns.\n",
        "    *   Optionally, you can provide a list of specific `values` to pivot on. This is highly recommended for performance and to ensure consistent schema, especially when the number of unique pivot values is large or dynamic.\n",
        "3.  **`agg(agg_function)`**: Apply an aggregation function to populate the new pivoted columns.\n",
        "\n",
        "### Why it's important for Data Engineers:\n",
        "\n",
        "*   **Report Generation**: Creating summary tables where categories are presented as columns (e.g., sales per product per region).\n",
        "*   **Data Reshaping**: Transforming data into a format suitable for specific analytical tools or models that expect a \"wide\" table.\n",
        "*   **Comparisons**: Easily compare metrics across different categories side-by-side.\n",
        "\n",
        "### Example (Python):"
      ],
      "metadata": {
        "id": "JUbWIQk-XII_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjG3vBAkEBxo",
        "outputId": "c1fee171-c53f-4b19-dc1b-50b3e51b187a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+-----+\n",
            "| Region| Product|Sales|\n",
            "+-------+--------+-----+\n",
            "|RegionA|ProductX|  100|\n",
            "|RegionA|ProductY|  150|\n",
            "|RegionB|ProductX|  200|\n",
            "|RegionB|ProductY|  120|\n",
            "|RegionA|ProductX|   50|\n",
            "|RegionC|ProductY|  300|\n",
            "+-------+--------+-----+\n",
            "\n",
            "\n",
            "--- Pivot on 'Product' to show sales per region per product ---\n",
            "+-------+--------+--------+\n",
            "| Region|ProductX|ProductY|\n",
            "+-------+--------+--------+\n",
            "|RegionB|     200|     120|\n",
            "|RegionA|     150|     150|\n",
            "|RegionC|    NULL|     300|\n",
            "+-------+--------+--------+\n",
            "\n",
            "\n",
            "--- Pivot with specific product values (recommended for performance/consistency) ---\n",
            "+-------+--------+--------+--------+\n",
            "| Region|ProductX|ProductY|ProductZ|\n",
            "+-------+--------+--------+--------+\n",
            "|RegionB|     200|     120|    NULL|\n",
            "|RegionA|     150|     150|    NULL|\n",
            "|RegionC|    NULL|     300|    NULL|\n",
            "+-------+--------+--------+--------+\n",
            "\n",
            "\n",
            "--- Pivoting with another aggregation (e.g., average) ---\n",
            "+-------+--------+--------+\n",
            "| Region|ProductX|ProductY|\n",
            "+-------+--------+--------+\n",
            "|RegionB|   200.0|   120.0|\n",
            "|RegionA|    75.0|   150.0|\n",
            "|RegionC|    NULL|   300.0|\n",
            "+-------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, avg\n",
        "\n",
        "# 1. Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"PivotOperations\").getOrCreate()\n",
        "\n",
        "# 2. Create Sample Data\n",
        "data = [(\"RegionA\", \"ProductX\", 100),\n",
        "        (\"RegionA\", \"ProductY\", 150),\n",
        "        (\"RegionB\", \"ProductX\", 200),\n",
        "        (\"RegionB\", \"ProductY\", 120),\n",
        "        (\"RegionA\", \"ProductX\", 50),\n",
        "        (\"RegionC\", \"ProductY\", 300)]\n",
        "columns = [\"Region\", \"Product\", \"Sales\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "print(\"\\n--- Pivot on 'Product' to show sales per region per product ---\")\n",
        "df.groupBy(\"Region\").pivot(\"Product\").agg(sum(\"Sales\")).show()\n",
        "\n",
        "print(\"\\n--- Pivot with specific product values (recommended for performance/consistency) ---\")\n",
        "# Explicitly specify pivot values to avoid potential issues with high cardinality\n",
        "# and ensure all desired columns are present even if a product has no sales in a region.\n",
        "df.groupBy(\"Region\").pivot(\"Product\", [\"ProductX\", \"ProductY\", \"ProductZ\"]).agg(sum(\"Sales\")).show()\n",
        "\n",
        "print(\"\\n--- Pivoting with another aggregation (e.g., average) ---\")\n",
        "df.groupBy(\"Region\").pivot(\"Product\").agg(avg(\"Sales\")).show()\n",
        "\n",
        "spark.stop()\n",
        "\n",
        "# Key Note:\n",
        "# Multiple aggregations with pivot() are not directly supported in a single call.\n",
        "# For multiple aggregates (e.g., sum and avg for each pivoted column),\n",
        "# you would typically perform separate pivots and then join the results,\n",
        "# or use advanced techniques like `cube` or `rollup` (which are different but related to multi-dimensional aggregations)."
      ]
    }
  ]
}