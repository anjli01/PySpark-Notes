{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/02_SparkSession_%26_SparkContext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparkSession & SparkContext: The Entry Points\n",
        "\n",
        "To interact with Spark, you use either a `SparkSession` (recommended for Spark 2.x+) or a `SparkContext`.\n",
        "\n",
        "### SparkSession.builder.getOrCreate()\n",
        "*   **Unified Entry Point:** The primary way to start using Spark from version 2.x onwards. It consolidates functionalities of `SparkContext`, `SQLContext`, `HiveContext`, and `StreamingContext`.\n",
        "*   **How it Works:**\n",
        "    *   `builder`: Returns a `SparkSession.Builder` object for configuring Spark properties.\n",
        "    *   `getOrCreate()`:\n",
        "        *   If a `SparkSession` instance already exists, it returns the existing one.\n",
        "        *   If no `SparkSession` exists, it creates a new one based on the builder's configuration.\n",
        "*   **Benefit:** Ensures you always have a single, active `SparkSession`, preventing conflicts.\n",
        "\n",
        "### Common SparkSession Configurations\n",
        "\n",
        "When building a `SparkSession`, you can configure various properties:\n",
        "\n",
        "| Method                 | Description                                                                                             | Example                                        |\n",
        "| :--------------------- | :------------------------------------------------------------------------------------------------------ | :--------------------------------------------- |\n",
        "| `appName(name)`        | Sets a name for your application, visible in the Spark UI.                                              | `.appName(\"MySparkApp\")`                       |\n",
        "| `master(url)`          | Specifies the master URL for the cluster.                                                               | `.master(\"local[*]\")`                          |\n",
        "| `config(key, value)`   | Sets a specific Spark configuration property.                                                           | `.config(\"spark.executor.memory\", \"2g\")`       |\n",
        "\n",
        "#### `master(url)` Options:\n",
        "\n",
        "*   `local`: Runs Spark locally with one thread.\n",
        "*   `local[*]`: Runs Spark locally with as many worker threads as logical cores on your machine.\n",
        "*   `local[N]`: Runs Spark locally with `N` worker threads.\n",
        "*   `spark://host:port`: Connects to a standalone Spark cluster.\n",
        "*   `yarn`: Connects to a YARN cluster.\n",
        "*   `mesos://host:port`: Connects to a Mesos cluster.\n",
        "\n",
        "#### Example (Python): Building a SparkSession"
      ],
      "metadata": {
        "id": "SDguVlm8WWXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmyt7kXXEGPZ",
        "outputId": "a78627e9-4e55-43c4-fa45-cf90c89e2209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession created successfully!\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Build a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"SparkSession created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of configuring more properties\n",
        "spark_configured = SparkSession.builder \\\n",
        "                               .appName(\"ConfiguredSparkApp\") \\\n",
        "                               .master(\"local[4]\") \\\n",
        "                               .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "                               .config(\"spark.driver.memory\", \"1g\") \\\n",
        "                               .getOrCreate()\n",
        "\n",
        "print(\"\\nSpark configuration:\")\n",
        "print(f\"App Name: {spark_configured.conf.get('spark.app.name')}\")\n",
        "print(f\"Master: {spark_configured.conf.get('spark.master')}\")\n",
        "print(f\"Shuffle Partitions: {spark_configured.conf.get('spark.sql.shuffle.partitions')}\")\n",
        "\n",
        "# Remember to stop the SparkSession when done\n",
        "spark.stop()\n",
        "spark_configured.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gedrVX1paWpM",
        "outputId": "a5a9a7cb-ac1e-43fd-caeb-f941aef8c140"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spark configuration:\n",
            "App Name: ConfiguredSparkApp\n",
            "Master: local[4]\n",
            "Shuffle Partitions: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Accessing SparkContext from SparkSession\n",
        "\n",
        "While `SparkSession` is the primary entry point, you can still access the underlying `SparkContext` instance.\n",
        "\n",
        "*   **How to Access:** Use `spark.sparkContext`.\n",
        "*   **Role of SparkContext:**\n",
        "    *   Connects to the Spark cluster.\n",
        "    *   Creates Resilient Distributed Datasets (RDDs) â€“ the foundational data structure in Spark 1.x.\n",
        "    *   Responsible for broadcasting variables (sending read-only data to all executors).\n",
        "*   **Usage for Beginners:** Primarily used for low-level RDD operations or when you need to broadcast data, though most modern Spark tasks can be handled directly by `SparkSession` with DataFrames.\n",
        "\n",
        "#### Example (Python): Accessing SparkContext\n"
      ],
      "metadata": {
        "id": "dsALH1XZWX1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AccessSparkContext\") \\\n",
        "    .master(\"local\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Access SparkContext from SparkSession\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"SparkContext application ID: {sc.applicationId}\")\n",
        "print(f\"SparkContext version: {sc.version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rCTV-bdWYgi",
        "outputId": "3475d5d9-1d8f-47e9-8790-635b0caaeb3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkContext application ID: local-1759410308814\n",
            "SparkContext version: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an RDD operation using sc\n",
        "rdd_data = sc.parallelize([10, 20, 30])\n",
        "print(\"RDD sum:\", rdd_data.sum())\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u31urpyuaSYC",
        "outputId": "0d4eb702-ded1-42ec-8dd2-8e770639782c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD sum: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Understanding Driver vs. Executors\n",
        "\n",
        "In a Spark cluster, two main components facilitate distributed processing: the **driver** and **executors**.\n",
        "\n",
        "### 1. Driver Program\n",
        "*(The \"Project Manager\")*\n",
        "\n",
        "*   **Location:** Runs on the driver node (the machine where you launched your Spark application).\n",
        "*   **Key Responsibilities:**\n",
        "    *   Contains the `SparkSession` and `SparkContext`.\n",
        "    *   Translates user code (DataFrame transformations) into a Directed Acyclic Graph (DAG).\n",
        "    *   Converts the DAG into an optimized physical plan (via Catalyst Optimizer).\n",
        "    *   Schedules tasks on executors.\n",
        "    *   Maintains information about the cluster (e.g., available executors).\n",
        "    *   Aggregates results from executors.\n",
        "\n",
        "### 2. Executors\n",
        "*(The \"Workers\")*\n",
        "\n",
        "*   **Location:** Worker processes that run on worker nodes in the cluster.\n",
        "*   **Key Responsibilities:**\n",
        "    *   Execute the actual tasks assigned by the driver.\n",
        "    *   Perform computations on their assigned partition of data.\n",
        "    *   Store data (if caching is involved).\n",
        "    *   Return results or status updates to the driver.\n",
        "    *   Each executor can run multiple tasks concurrently.\n",
        "\n",
        "### Communication Flow\n",
        "\n",
        "1.  **User Code to Logical Plan:** Your Spark application code (e.g., DataFrame transformations) is translated into a logical plan by the driver.\n",
        "2.  **Logical to Physical Plan:** The Catalyst Optimizer converts the logical plan into an optimized physical plan.\n",
        "3.  **Task Scheduling:** The driver breaks down the physical plan into stages and then into individual tasks, distributing them to available executors.\n",
        "4.  **Task Execution:** Executors receive tasks, process their assigned portion of the data, and return results or status updates to the driver.\n",
        "\n",
        "This distributed architecture allows Spark to process vast amounts of data in parallel, making it highly scalable."
      ],
      "metadata": {
        "id": "1NkyBT98WZP7"
      }
    }
  ]
}