{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/07_Reading_%26_Writing_Files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading & Writing Files\n",
        "\n",
        "Spark provides robust capabilities for interacting with various file formats, which is fundamental for data ingestion and persistence.\n",
        "\n",
        "#### File Formats: CSV, JSON, Parquet\n",
        "\n",
        "Spark supports a multitude of file formats, with **CSV**, **JSON**, and **Parquet** being among the most common.\n",
        "\n",
        "**Comparison of Common File Formats in Spark:**\n",
        "\n",
        "| Feature                | CSV (Comma Separated Values)                                  | JSON (JavaScript Object Notation)                                   | Parquet                                                               |\n",
        "| :--------------------- | :------------------------------------------------------------ | :------------------------------------------------------------------ | :-------------------------------------------------------------------- |\n",
        "| **Data Structure**     | Row-based, plain text                                         | Semi-structured, row-based, plain text                              | **Columnar**, binary, self-describing                                 |\n",
        "| **Human-Readable**     | Yes                                                           | Yes                                                                 | No (requires tools/Spark to read)                                     |\n",
        "| **Schema Enforcement** | No (schema must be inferred or provided)                      | Flexible (can be error-prone with inconsistent data)                | **Yes** (built-in schema stored with data)                            |\n",
        "| **Compression**        | No built-in compression (can be applied externally)           | No built-in compression (can be applied externally)                 | **Highly efficient built-in compression and encoding**                |\n",
        "| **Nested Data**        | Difficult/impossible to represent                               | Good support for nested structures                                  | **Excellent support** for complex nested data                         |\n",
        "| **Read Performance**   | Poor for large datasets (requires full scan for schema infer, row-by-row parsing) | Moderate (parsing overhead, still row-by-row processing)          | **Excellent** for analytical queries (reads only necessary columns, predicate pushdown) |\n",
        "| **Schema Evolution**   | Poor                                                          | Fair (flexible schema, but requires careful handling)               | **Excellent** (built-in support)                                      |\n",
        "| **Use Cases**          | Small data, ad-hoc analysis, data exchange with other systems | Log data, API responses, flexible data interchange                  | **Recommended for big data storage & analytics**, ETL intermediate stages, Data Lakes |\n",
        "| **Spark Optimization** | Limited                                                       | Limited                                                             | **Highly optimized** for Spark and other columnar engines             |\n",
        "\n",
        "---\n",
        "\n",
        "#### Read Options:\n",
        "\n",
        "When reading files, you can specify various options to control how Spark interprets the data:\n",
        "\n",
        "*   **`header`**: (`True`/`False`) Indicates if the first row is a header (Default: `False`).\n",
        "*   **`inferSchema`**: (`True`/`False`)\n",
        "    *   `True`: Spark will read a sample of the data to infer data types. **Caution**: Can be slow for large files as it requires an additional pass over the data.\n",
        "    *   `False`: All columns will be read as `StringType`.\n",
        "    *   **Best Practice**: For production, always define schemas manually (`StructType`) for performance and reliability.\n",
        "*   **`multiline`**: (`True`/`False`) (Primarily for JSON)\n",
        "    *   `True`: Reads a single JSON object that spans multiple lines.\n",
        "    *   `False`: Expects one JSON object per line. (Default: `False` for JSON).\n",
        "*   **`sep`**: Specifies the column delimiter for CSV files (Default: `,`).\n",
        "*   **`compression`**: Codec to use for compression (e.g., `gzip`, `snappy`, `lz4`, `bzip2`). Spark often infers this from the file extension.\n",
        "\n",
        "---\n",
        "\n",
        "**Example (Python): Reading Files**\n"
      ],
      "metadata": {
        "id": "T8ZbqZoiJVHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadWriteFiles\").getOrCreate()\n",
        "\n",
        "# Create dummy files for demonstration\n",
        "# CSV data\n",
        "csv_data = \"\"\"id|name|age\n",
        "1|Alice|30\n",
        "2|Bob|25\n",
        "3|Charlie|35\n",
        "\"\"\"\n",
        "with open(\"data.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "# JSON (multi-line record - simplified to single line for example, but `multiline=True` would handle real multi-line)\n",
        "json_data = \"\"\"{\"id\":1,\"name\":\"Alice\", \"details\":{\"age\":30, \"city\":\"NY\"}}\n",
        "{\"id\":2,\"name\":\"Bob\", \"details\":{\"age\":25, \"city\":\"LD\"}}\n",
        "\"\"\"\n",
        "with open(\"data.json\", \"w\") as f:\n",
        "    f.write(json_data)\n",
        "\n",
        "# --- CSV Read Options ---\n",
        "print(\"Reading CSV with header and custom separator:\")\n",
        "df_csv = spark.read.csv(\"data.csv\", header=True, inferSchema=True, sep=\"|\")\n",
        "df_csv.printSchema()\n",
        "df_csv.show()\n",
        "\n",
        "# --- JSON Read Options ---\n",
        "print(\"\\nReading JSON with multiline=False (default, for line-delimited JSON):\")\n",
        "# Note: Even if `json_data` was truly multiline for a single object, multiline=False treats each line as a record.\n",
        "# To read a single JSON object spanning multiple lines, `multiline=True` would be used.\n",
        "df_json = spark.read.json(\"data.json\")\n",
        "df_json.printSchema()\n",
        "df_json.show(truncate=False)\n",
        "\n",
        "# --- Demonstrate Compression (Write then Read) ---\n",
        "# Write a gzipped CSV file\n",
        "df_csv.write.option(\"compression\", \"gzip\").csv(\"data_compressed.csv.gz\",\n",
        "                                              mode=\"overwrite\", header=True)\n",
        "print(\"\\nReading compressed CSV:\")\n",
        "df_compressed = spark.read.option(\"compression\", \"gzip\").csv(\"data_compressed.csv.gz\",\n",
        "                                                            header=True, inferSchema=True)\n",
        "df_compressed.printSchema()\n",
        "df_compressed.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvV9J7Wrg2XX",
        "outputId": "73123448-783d-43e3-a054-644e67989b3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV with header and custom separator:\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 30|\n",
            "|  2|    Bob| 25|\n",
            "|  3|Charlie| 35|\n",
            "+---+-------+---+\n",
            "\n",
            "\n",
            "Reading JSON with multiline=False (default, for line-delimited JSON):\n",
            "root\n",
            " |-- details: struct (nullable = true)\n",
            " |    |-- age: long (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+--------+---+-----+\n",
            "|details |id |name |\n",
            "+--------+---+-----+\n",
            "|{30, NY}|1  |Alice|\n",
            "|{25, LD}|2  |Bob  |\n",
            "+--------+---+-----+\n",
            "\n",
            "\n",
            "Reading compressed CSV:\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 30|\n",
            "|  2|    Bob| 25|\n",
            "|  3|Charlie| 35|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Write Options:\n",
        "\n",
        "When writing DataFrames to files, you have control over the output behavior:\n",
        "\n",
        "*   **`mode`**: Specifies the save mode for existing data.\n",
        "    *   `overwrite`: Overwrites existing data/directory.\n",
        "    *   `append`: Appends new data to existing data.\n",
        "    *   `ignore`: If data already exists, the write operation does nothing.\n",
        "    *   `errorIfExists` (Default): Throws an error if data already exists.\n",
        "*   **`partitionBy(column_names)`**:\n",
        "    *   Partitions the output data by the values of specified columns.\n",
        "    *   Creates subdirectories in the output path (e.g., `output/city=NY/`, `output/city=LD/`).\n",
        "    *   **Benefit**: Improves query performance by allowing Spark to skip scanning irrelevant partitions (Predicate Pushdown).\n",
        "*   **`bucketBy(num_buckets, column_names)`**:\n",
        "    *   Buckets the output data by hashing the specified columns into a fixed number of buckets.\n",
        "    *   **Benefit**: Improves join performance (co-located data for joins) and sampling.\n",
        "    *   **Requirement**: Requires saving as a table (e.g., Hive table).\n",
        "*   **`.option(key, value)` vs `.options(**kwargs)`**:\n",
        "    *   `.option(key, value)`: Sets a single write option.\n",
        "    *   `.options(**kwargs)`: Sets multiple options using keyword arguments (Python-specific).\n",
        "\n",
        "---\n",
        "\n",
        "**Example (Python): Writing Files**"
      ],
      "metadata": {
        "id": "7sOLHBd0W_wB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nmqzSc43EC5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51193ba6-b77e-471d-fc4d-b0218a418dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----+\n",
            "|   Name|Age|City|\n",
            "+-------+---+----+\n",
            "|  Alice| 30|  NY|\n",
            "|    Bob| 25|  LD|\n",
            "|Charlie| 35|  NY|\n",
            "|  David| 22|  SF|\n",
            "+-------+---+----+\n",
            "\n",
            "\n",
            "Writing CSV with overwrite mode...\n",
            "\n",
            "Writing Parquet partitioned by 'City'...\n",
            "\n",
            "Writing JSON with append mode...\n",
            "\n",
            "Writing CSV using .options():\n",
            "\n",
            "Write operations completed. Check your 'output_data' directory.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WriteOptions\").getOrCreate()\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = [(\"Alice\", 30, \"NY\"), (\"Bob\", 25, \"LD\"), (\"Charlie\", 35, \"NY\"), (\"David\", 22, \"SF\")]\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "output_base_path = \"output_data\"\n",
        "\n",
        "# --- Write to CSV with overwrite mode ---\n",
        "print(\"\\nWriting CSV with overwrite mode...\")\n",
        "# This will create 'output_data/csv_output_overwrite' directory\n",
        "df.write.mode(\"overwrite\").csv(f\"{output_base_path}/csv_output_overwrite\", header=True)\n",
        "\n",
        "# --- Write to Parquet, partitioned by 'City' ---\n",
        "print(\"\\nWriting Parquet partitioned by 'City'...\")\n",
        "# This will create directories like 'output_data/parquet_partitioned/City=NY/'\n",
        "df.write.mode(\"overwrite\").partitionBy(\"City\").parquet(f\"{output_base_path}/parquet_partitioned\")\n",
        "\n",
        "# --- Write to JSON with append mode (demonstrating mode) ---\n",
        "# For append, run this block twice. The first time it creates the file.\n",
        "# The second time, it appends data to the existing JSON file.\n",
        "print(\"\\nWriting JSON with append mode...\")\n",
        "df.write.mode(\"append\").json(f\"{output_base_path}/json_output_append\")\n",
        "\n",
        "# --- Using .options() (Python specific for multiple options) ---\n",
        "print(\"\\nWriting CSV using .options():\")\n",
        "df.write.mode(\"overwrite\").options(header=True, sep=\",\").csv(f\"{output_base_path}/csv_output_options\")\n",
        "\n",
        "print(f\"\\nWrite operations completed. Check your '{output_base_path}' directory.\")\n",
        "\n",
        "spark.stop()"
      ]
    }
  ]
}