{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjli01/PySpark-Notes/blob/main/16_Repartition_vs_Coalesce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repartition vs. Coalesce\n",
        "\n",
        "These functions control the number of partitions in a DataFrame, impacting parallelism and performance. The key difference is whether they trigger a full data shuffle.\n",
        "\n",
        "#### `repartition(n)` — Increases/Decreases Partitions with Shuffle\n",
        "\n",
        "*   **Purpose**: Redistributes data across `n` partitions, ensuring even distribution across cluster nodes.\n",
        "*   **Behavior**: *Always* involves a **full shuffle** of data. Each row can move to any partition.\n",
        "*   **Use Cases**:\n",
        "    *   **Increasing Parallelism**: When current partitions are too few for CPU-bound tasks.\n",
        "    *   **Even Distribution**: Decreasing partitions while ensuring balanced data distribution.\n",
        "    *   **Balancing Data Skew**: Rebalancing data when some partitions are much larger than others.\n",
        "    *   **Changing Partitioning Key**: To optimize future joins or aggregations by repartitioning by specific columns.\n",
        "*   **Overhead**: High network I/O and disk I/O due to the full shuffle.\n",
        "\n",
        "#### `coalesce(n)` — Decreases Partitions Without Full Shuffle\n",
        "\n",
        "*   **Purpose**: Decreases the number of partitions.\n",
        "*   **Behavior**: Attempts to combine existing partitions on the same nodes. **Avoids a full shuffle if possible** by merging partitions. It cannot increase the number of partitions beyond the current count. If `n` is greater than the current number of partitions, it simply returns the current number.\n",
        "*   **Use Cases**:\n",
        "    *   **Reducing Partitions for Writing**: When writing to a single file or a small number of files to avoid creating many tiny files (which are inefficient in distributed file systems).\n",
        "    *   **Minimizing Shuffle**: When you need to reduce partitions, but want to avoid the high cost of a full shuffle.\n",
        "*   **Overhead**: Lower overhead than `repartition()` as it avoids a full shuffle.\n",
        "\n",
        "#### Example Code"
      ],
      "metadata": {
        "id": "A0QB7PnOJpXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFRVneXsD8BX",
        "outputId": "ac625360-dd61-44ff-bd04-1212b2e36623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial partitions: 10\n",
            "Partitions after repartition(10): 10\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ShuffleQueryStage 0\n",
            "   +- Exchange RoundRobinPartitioning(10), REPARTITION_BY_NUM, [plan_id=15]\n",
            "      +- *(1) Scan ExistingRDD[value#0L]\n",
            "+- == Initial Plan ==\n",
            "   Exchange RoundRobinPartitioning(10), REPARTITION_BY_NUM, [plan_id=10]\n",
            "   +- Scan ExistingRDD[value#0L]\n",
            "\n",
            "\n",
            "Partitions after repartition(2): 2\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ShuffleQueryStage 0\n",
            "   +- Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=31]\n",
            "      +- *(1) Scan ExistingRDD[value#0L]\n",
            "+- == Initial Plan ==\n",
            "   Exchange RoundRobinPartitioning(2), REPARTITION_BY_NUM, [plan_id=26]\n",
            "   +- Scan ExistingRDD[value#0L]\n",
            "\n",
            "\n",
            "Partitions after coalesce(5): 5\n",
            "== Physical Plan ==\n",
            "Coalesce 5\n",
            "+- *(1) Scan ExistingRDD[value#0L]\n",
            "\n",
            "\n",
            "Partitions after coalesce(1): 1\n",
            "Partitions after coalesce(15) (will not increase): 10\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RepartitionCoalesce\").getOrCreate()\n",
        "\n",
        "data = [(i,) for i in range(100)] # 100 rows\n",
        "df = spark.createDataFrame(spark.sparkContext.parallelize(data, 10), [\"value\"]) # Start with 10 partitions\n",
        "\n",
        "print(f\"Initial partitions: {df.rdd.getNumPartitions()}\") # Should be 10 (or Spark's default if not specified)\n",
        "\n",
        "# Repartition to 10 partitions (no change if starting with 10, but demonstrates repartition)\n",
        "df_repartitioned = df.repartition(10)\n",
        "print(f\"Partitions after repartition(10): {df_repartitioned.rdd.getNumPartitions()}\")\n",
        "df_repartitioned.explain() # Look for 'Exchange' for shuffle\n",
        "\n",
        "# Repartition to 2 partitions\n",
        "df_repartitioned_small = df.repartition(2)\n",
        "print(f\"Partitions after repartition(2): {df_repartitioned_small.rdd.getNumPartitions()}\")\n",
        "df_repartitioned_small.explain() # Look for 'Exchange'\n",
        "\n",
        "# Coalesce to 5 partitions (decreases without full shuffle)\n",
        "df_coalesced = df.coalesce(5)\n",
        "print(f\"Partitions after coalesce(5): {df_coalesced.rdd.getNumPartitions()}\")\n",
        "df_coalesced.explain() # Should not show 'Exchange' for valid decreases\n",
        "\n",
        "# Coalesce to 1 partition (useful for writing a single file)\n",
        "df_coalesced_single = df.coalesce(1)\n",
        "print(f\"Partitions after coalesce(1): {df_coalesced_single.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Try to coalesce to a number higher than initial partitions - it won't increase\n",
        "df_coalesced_larger = df.coalesce(15)\n",
        "print(f\"Partitions after coalesce(15) (will not increase): {df_coalesced_larger.rdd.getNumPartitions()}\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### When to Use Which: Comparison Table\n",
        "\n",
        "| Feature         | `repartition(n)`                                | `coalesce(n)`                                                  |\n",
        "| :-------------- | :---------------------------------------------- | :------------------------------------------------------------- |\n",
        "| **Shuffle**     | **Always** triggers a full data shuffle.        | Avoids a full shuffle *if possible* (merges existing partitions). |\n",
        "| **Partitions**  | Can increase or decrease partitions.            | Can only decrease or maintain partitions (or return original if `n` is higher than current). |\n",
        "| **Distribution**| Guarantees even distribution of data.           | May result in unevenly sized partitions.                       |\n",
        "| **Performance** | Higher overhead due to network I/O and disk I/O. | Lower overhead, faster for reducing partitions.               |\n",
        "| **Use Cases**   | - Increasing parallelism<br>- Balancing skewed data<br>- Changing partitioning key for joins | - Reducing number of small files when writing<br>- Minimizing network I/O when decreasing partitions |\n",
        "\n",
        "#### General Guidelines and Best Practices\n",
        "\n",
        "*   **Write Optimization**: When writing a large DataFrame to a few files (e.g., one file), use `coalesce(1)` or `coalesce(N)` (where N is small) to prevent many tiny output files.\n",
        "*   **Shuffle Control**: Be mindful of `repartition()`; it's an expensive operation. Use it only when a full data redistribution is absolutely necessary.\n",
        "*   **Increasing Parallelism**: If tasks are CPU-bound and your current number of partitions is too low, use `repartition()` to boost parallelism.\n",
        "*   **Data Skew**: If the Spark UI shows data skew (some tasks taking much longer), `repartition()` by the skewed column(s) can help redistribute data evenly.\n",
        "*   **Best Practice**: Understand your data and workload. Monitor Spark UI (tasks, durations, shuffle read/write bytes) to determine if your partitioning strategy is optimal.\n"
      ],
      "metadata": {
        "id": "Sm5nFHUoXl3A"
      }
    }
  ]
}